{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Curso Bônus - Data Science e Machine Learning com Linguagem Julia</font>\n",
    "\n",
    "## <font color='blue'>Machine Learning com Linguagem Julia</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagens/MP-MLJulia.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `C:\\Users\\david\\Desktop\\Formacao_Engenheiro_inteligencia_artificial\\8-DS_e_ML_com_Julia\\JuliaLanguage\\env`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → AccessorsDatesExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLatinHypercubeSampling\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mUnsafeAtomicsLLVM\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSetfield\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → AccessorsTestExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → AccessorsStaticArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOpenML\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatsFuns → StatsFunsChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSplittablesBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangStaticArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangTablesExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMicroCollections\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mKernelAbstractions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mKernelAbstractions → LinearAlgebraExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mKernelAbstractions → SparseArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mTransducers\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mTransducers → TransducersAdaptExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDistributions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDistributions → DistributionsChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDistributions → DistributionsTestExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mScientificTypes\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNNlib\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCategoricalDistributions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLStyle\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mJuliaVariables\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJModels\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFLoops\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLUtils\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatisticalMeasuresBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJEnsembles\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatisticalMeasures\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatisticalMeasures → ScientificTypesExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJBase → DefaultMeasuresExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJTuning\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJBalancing\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJFlow\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJIteration\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mMLJ\n",
      "  41 dependencies successfully precompiled in 87 seconds. 133 already precompiled.\n"
     ]
    }
   ],
   "source": [
    "# Cria e instancia um env\n",
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\david\\Desktop\\Formacao_Engenheiro_inteligencia_artificial\\8-DS_e_ML_com_Julia\\JuliaLanguage\\env\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\david\\Desktop\\Formacao_Engenheiro_inteligencia_artificial\\8-DS_e_ML_com_Julia\\JuliaLanguage\\env\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "# Instala o pacote MLJ\n",
    "Pkg.add(\"MLJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa o pacote\n",
    "using MLJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\textbf{Author}: R.A. Fisher   \\textbf{Source}: \\href{https://archive.ics.uci.edu/ml/datasets/Iris}{UCI} - 1936 - Donated by Michael Marshall   \\textbf{Please cite}:   \n",
       "\n",
       "\\textbf{Iris Plants Database}   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda \\& Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "Predicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n",
       "\n",
       "\\subsubsection{Attribute Information:}\n",
       "\\begin{verbatim}\n",
       "1. sepal length in cm\n",
       "2. sepal width in cm\n",
       "3. petal length in cm\n",
       "4. petal width in cm\n",
       "5. class: \n",
       "   -- Iris Setosa\n",
       "   -- Iris Versicolour\n",
       "   -- Iris Virginica\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "**Author**: R.A. Fisher   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall   **Please cite**:   \n",
       "\n",
       "**Iris Plants Database**   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "Predicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n",
       "\n",
       "### Attribute Information:\n",
       "\n",
       "```\n",
       "1. sepal length in cm\n",
       "2. sepal width in cm\n",
       "3. petal length in cm\n",
       "4. petal width in cm\n",
       "5. class: \n",
       "   -- Iris Setosa\n",
       "   -- Iris Versicolour\n",
       "   -- Iris Virginica\n",
       "```\n"
      ],
      "text/plain": [
       "  \u001b[1mAuthor\u001b[22m: R.A. Fisher \u001b[1mSource\u001b[22m: UCI\n",
       "  (https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael\n",
       "  Marshall \u001b[1mPlease cite\u001b[22m:\n",
       "\n",
       "  \u001b[1mIris Plants Database\u001b[22m This is perhaps the best known database to be found in\n",
       "  the pattern recognition literature. Fisher's paper is a classic in the field\n",
       "  and is referenced frequently to this day. (See Duda & Hart, for example.)\n",
       "  The data set contains 3 classes of 50 instances each, where each class\n",
       "  refers to a type of iris plant. One class is linearly separable from the\n",
       "  other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "  Predicted attribute: class of iris plant. This is an exceedingly simple\n",
       "  domain.\n",
       "\n",
       "\u001b[1m  Attribute Information:\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[36m  1. sepal length in cm\u001b[39m\n",
       "\u001b[36m  2. sepal width in cm\u001b[39m\n",
       "\u001b[36m  3. petal length in cm\u001b[39m\n",
       "\u001b[36m  4. petal width in cm\u001b[39m\n",
       "\u001b[36m  5. class: \u001b[39m\n",
       "\u001b[36m     -- Iris Setosa\u001b[39m\n",
       "\u001b[36m     -- Iris Versicolour\u001b[39m\n",
       "\u001b[36m     -- Iris Virginica\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descreve o dataset 61\n",
    "OpenML.describe_dataset(61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mDownloading dataset 61.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tables.DictColumnTable with 150 rows, 5 columns, and schema:\n",
       " :sepallength  Float64\n",
       " :sepalwidth   Float64\n",
       " :petallength  Float64\n",
       " :petalwidth   Float64\n",
       " :class        CategoricalArrays.CategoricalValue{String, UInt32}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega o dataset 61\n",
    "iris = OpenML.load(61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling CategoricalArraysSentinelArraysExt [25d60f2f-6134-5c9e-b13a-96ac465bddcb]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling BangBangDataFramesExt [d787bcad-b5c5-56bb-adaa-6bfddb178a59]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling TransducersDataFramesExt [cefb4096-3352-5e5f-8501-71f024082a88]\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>150×5 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">125 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">sepallength</th><th style = \"text-align: left;\">sepalwidth</th><th style = \"text-align: left;\">petallength</th><th style = \"text-align: left;\">petalwidth</th><th style = \"text-align: left;\">class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"CategoricalArrays.CategoricalValue{String, UInt32}\" style = \"text-align: left;\">Cat…</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">3.5</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.9</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">4.7</td><td style = \"text-align: right;\">3.2</td><td style = \"text-align: right;\">1.3</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">4.6</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">3.6</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">3.9</td><td style = \"text-align: right;\">1.7</td><td style = \"text-align: right;\">0.4</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">4.6</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.3</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">4.4</td><td style = \"text-align: right;\">2.9</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">4.9</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.1</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">3.7</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">4.8</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">1.6</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">4.8</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.1</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">139</td><td style = \"text-align: right;\">6.0</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">4.8</td><td style = \"text-align: right;\">1.8</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">140</td><td style = \"text-align: right;\">6.9</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">2.1</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">141</td><td style = \"text-align: right;\">6.7</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">5.6</td><td style = \"text-align: right;\">2.4</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">142</td><td style = \"text-align: right;\">6.9</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">143</td><td style = \"text-align: right;\">5.8</td><td style = \"text-align: right;\">2.7</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">144</td><td style = \"text-align: right;\">6.8</td><td style = \"text-align: right;\">3.2</td><td style = \"text-align: right;\">5.9</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">145</td><td style = \"text-align: right;\">6.7</td><td style = \"text-align: right;\">3.3</td><td style = \"text-align: right;\">5.7</td><td style = \"text-align: right;\">2.5</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">146</td><td style = \"text-align: right;\">6.7</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">5.2</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">147</td><td style = \"text-align: right;\">6.3</td><td style = \"text-align: right;\">2.5</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">148</td><td style = \"text-align: right;\">6.5</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">5.2</td><td style = \"text-align: right;\">2.0</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">149</td><td style = \"text-align: right;\">6.2</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">150</td><td style = \"text-align: right;\">5.9</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">1.8</td><td style = \"text-align: left;\">Iris-virginica</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& sepallength & sepalwidth & petallength & petalwidth & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t5 & 5.0 & 3.6 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t6 & 5.4 & 3.9 & 1.7 & 0.4 & Iris-setosa \\\\\n",
       "\t7 & 4.6 & 3.4 & 1.4 & 0.3 & Iris-setosa \\\\\n",
       "\t8 & 5.0 & 3.4 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t9 & 4.4 & 2.9 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t10 & 4.9 & 3.1 & 1.5 & 0.1 & Iris-setosa \\\\\n",
       "\t11 & 5.4 & 3.7 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t12 & 4.8 & 3.4 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t13 & 4.8 & 3.0 & 1.4 & 0.1 & Iris-setosa \\\\\n",
       "\t14 & 4.3 & 3.0 & 1.1 & 0.1 & Iris-setosa \\\\\n",
       "\t15 & 5.8 & 4.0 & 1.2 & 0.2 & Iris-setosa \\\\\n",
       "\t16 & 5.7 & 4.4 & 1.5 & 0.4 & Iris-setosa \\\\\n",
       "\t17 & 5.4 & 3.9 & 1.3 & 0.4 & Iris-setosa \\\\\n",
       "\t18 & 5.1 & 3.5 & 1.4 & 0.3 & Iris-setosa \\\\\n",
       "\t19 & 5.7 & 3.8 & 1.7 & 0.3 & Iris-setosa \\\\\n",
       "\t20 & 5.1 & 3.8 & 1.5 & 0.3 & Iris-setosa \\\\\n",
       "\t21 & 5.4 & 3.4 & 1.7 & 0.2 & Iris-setosa \\\\\n",
       "\t22 & 5.1 & 3.7 & 1.5 & 0.4 & Iris-setosa \\\\\n",
       "\t23 & 4.6 & 3.6 & 1.0 & 0.2 & Iris-setosa \\\\\n",
       "\t24 & 5.1 & 3.3 & 1.7 & 0.5 & Iris-setosa \\\\\n",
       "\t25 & 4.8 & 3.4 & 1.9 & 0.2 & Iris-setosa \\\\\n",
       "\t26 & 5.0 & 3.0 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t27 & 5.0 & 3.4 & 1.6 & 0.4 & Iris-setosa \\\\\n",
       "\t28 & 5.2 & 3.5 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t29 & 5.2 & 3.4 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t30 & 4.7 & 3.2 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m150×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class          \u001b[0m\n",
       "     │\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…           \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────────────\n",
       "   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n",
       "   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n",
       "   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n",
       "   4 │         4.6         3.1          1.5         0.2  Iris-setosa\n",
       "   5 │         5.0         3.6          1.4         0.2  Iris-setosa\n",
       "   6 │         5.4         3.9          1.7         0.4  Iris-setosa\n",
       "   7 │         4.6         3.4          1.4         0.3  Iris-setosa\n",
       "   8 │         5.0         3.4          1.5         0.2  Iris-setosa\n",
       "   9 │         4.4         2.9          1.4         0.2  Iris-setosa\n",
       "  10 │         4.9         3.1          1.5         0.1  Iris-setosa\n",
       "  11 │         5.4         3.7          1.5         0.2  Iris-setosa\n",
       "  ⋮  │      ⋮           ⋮            ⋮           ⋮             ⋮\n",
       " 141 │         6.7         3.1          5.6         2.4  Iris-virginica\n",
       " 142 │         6.9         3.1          5.1         2.3  Iris-virginica\n",
       " 143 │         5.8         2.7          5.1         1.9  Iris-virginica\n",
       " 144 │         6.8         3.2          5.9         2.3  Iris-virginica\n",
       " 145 │         6.7         3.3          5.7         2.5  Iris-virginica\n",
       " 146 │         6.7         3.0          5.2         2.3  Iris-virginica\n",
       " 147 │         6.3         2.5          5.0         1.9  Iris-virginica\n",
       " 148 │         6.5         3.0          5.2         2.0  Iris-virginica\n",
       " 149 │         6.2         3.4          5.4         2.3  Iris-virginica\n",
       " 150 │         5.9         3.0          5.1         1.8  Iris-virginica\n",
       "\u001b[36m                                                        129 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converte o dataset em dataframe\n",
    "df = DataFrames.DataFrame(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>4×5 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">sepallength</th><th style = \"text-align: left;\">sepalwidth</th><th style = \"text-align: left;\">petallength</th><th style = \"text-align: left;\">petalwidth</th><th style = \"text-align: left;\">class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"CategoricalArrays.CategoricalValue{String, UInt32}\" style = \"text-align: left;\">Cat…</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">3.5</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.9</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">4.7</td><td style = \"text-align: right;\">3.2</td><td style = \"text-align: right;\">1.3</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">4.6</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& sepallength & sepalwidth & petallength & petalwidth & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class       \u001b[0m\n",
       "     │\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…        \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────\n",
       "   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n",
       "   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n",
       "   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n",
       "   4 │         4.6         3.1          1.5         0.2  Iris-setosa"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza as 4 primeiras linhas\n",
    "first(df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬───────────────┬──────────────────────────────────┐\n",
       "│\u001b[22m names       \u001b[0m│\u001b[22m scitypes      \u001b[0m│\u001b[22m types                            \u001b[0m│\n",
       "├─────────────┼───────────────┼──────────────────────────────────┤\n",
       "│ sepallength │ Continuous    │ Float64                          │\n",
       "│ sepalwidth  │ Continuous    │ Float64                          │\n",
       "│ petallength │ Continuous    │ Float64                          │\n",
       "│ petalwidth  │ Continuous    │ Float64                          │\n",
       "│ class       │ Multiclass{3} │ CategoricalValue{String, UInt32} │\n",
       "└─────────────┴───────────────┴──────────────────────────────────┘\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema do dataframe (metadados)\n",
    "schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalArrays.CategoricalValue{String, UInt32}[\"Iris-virginica\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-versicolor\", \"Iris-setosa\", \"Iris-virginica\"  …  \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-setosa\"], \u001b[1m150×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\n",
       "     │\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────\n",
       "   1 │         6.7         3.3          5.7         2.1\n",
       "   2 │         5.7         2.8          4.1         1.3\n",
       "   3 │         7.2         3.0          5.8         1.6\n",
       "   4 │         4.4         2.9          1.4         0.2\n",
       "   5 │         5.6         2.5          3.9         1.1\n",
       "   6 │         6.5         3.0          5.2         2.0\n",
       "   7 │         4.4         3.0          1.3         0.2\n",
       "   8 │         6.1         2.9          4.7         1.4\n",
       "   9 │         5.4         3.9          1.7         0.4\n",
       "  10 │         4.9         2.5          4.5         1.7\n",
       "  11 │         6.3         2.5          4.9         1.5\n",
       "  ⋮  │      ⋮           ⋮            ⋮           ⋮\n",
       " 141 │         6.4         2.7          5.3         1.9\n",
       " 142 │         6.8         3.2          5.9         2.3\n",
       " 143 │         6.9         3.1          5.4         2.1\n",
       " 144 │         6.1         2.8          4.0         1.3\n",
       " 145 │         6.7         2.5          5.8         1.8\n",
       " 146 │         5.0         3.5          1.3         0.3\n",
       " 147 │         7.6         3.0          6.6         2.1\n",
       " 148 │         6.3         2.5          5.0         1.9\n",
       " 149 │         5.1         3.8          1.6         0.2\n",
       " 150 │         5.0         3.6          1.4         0.2\n",
       "\u001b[36m                                        129 rows omitted\u001b[0m)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrai x e y do dataframe\n",
    "# x = variáveis preditoras\n",
    "# y = variavel alvo (class)\n",
    "y, X = unpack(df, ==(:class), rng = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AbstractVector{Multiclass{3}}\u001b[90m (alias for \u001b[39m\u001b[90mAbstractArray{Multiclass{3}, 1}\u001b[39m\u001b[90m)\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema da variável y\n",
    "scitype(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "unpack(table, f1, f2, ... fk;\n",
       "       wrap_singles=false,\n",
       "       shuffle=false,\n",
       "       rng::Union{AbstractRNG,Int,Nothing}=nothing,\n",
       "       coerce_options...)\n",
       "\\end{verbatim}\n",
       "Horizontally split any Tables.jl compatible \\texttt{table} into smaller tables or vectors by making column selections determined by the predicates \\texttt{f1}, \\texttt{f2}, ..., \\texttt{fk}. Selection from the column names is without replacement. A \\emph{predicate} is any object \\texttt{f} such that \\texttt{f(name)} is \\texttt{true} or \\texttt{false} for each column \\texttt{name::Symbol} of \\texttt{table}.\n",
       "\n",
       "Returns a tuple of tables/vectors with length one greater than the number of supplied predicates, with the last component including all previously unselected columns.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\n",
       "2×4 DataFrame\n",
       " Row │ x      y     z        w\n",
       "     │ Int64  Char  Float64  String\n",
       "─────┼──────────────────────────────\n",
       "   1 │     1  a        10.0  A\n",
       "   2 │     2  b        20.0  B\n",
       "\n",
       "julia> Z, XY, W = unpack(table, ==(:z), !=(:w));\n",
       "julia> Z\n",
       "2-element Vector{Float64}:\n",
       " 10.0\n",
       " 20.0\n",
       "\n",
       "julia> XY\n",
       "2×2 DataFrame\n",
       " Row │ x      y\n",
       "     │ Int64  Char\n",
       "─────┼─────────────\n",
       "   1 │     1  a\n",
       "   2 │     2  b\n",
       "\n",
       "julia> W  # the column(s) left over\n",
       "2-element Vector{String}:\n",
       " \"A\"\n",
       " \"B\"\n",
       "\\end{verbatim}\n",
       "Whenever a returned table contains a single column, it is converted to a vector unless \\texttt{wrap\\_singles=true}.\n",
       "\n",
       "If \\texttt{coerce\\_options} are specified then \\texttt{table} is first replaced with \\texttt{coerce(table, coerce\\_options)}. See \\href{@ref}{\\texttt{ScientificTypes.coerce}} for details.\n",
       "\n",
       "If \\texttt{shuffle=true} then the rows of \\texttt{table} are first shuffled, using the global RNG, unless \\texttt{rng} is specified; if \\texttt{rng} is an integer, it specifies the seed of an automatically generated Mersenne twister. If \\texttt{rng} is specified then \\texttt{shuffle=true} is implicit.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "unpack(table, f1, f2, ... fk;\n",
       "       wrap_singles=false,\n",
       "       shuffle=false,\n",
       "       rng::Union{AbstractRNG,Int,Nothing}=nothing,\n",
       "       coerce_options...)\n",
       "```\n",
       "\n",
       "Horizontally split any Tables.jl compatible `table` into smaller tables or vectors by making column selections determined by the predicates `f1`, `f2`, ..., `fk`. Selection from the column names is without replacement. A *predicate* is any object `f` such that `f(name)` is `true` or `false` for each column `name::Symbol` of `table`.\n",
       "\n",
       "Returns a tuple of tables/vectors with length one greater than the number of supplied predicates, with the last component including all previously unselected columns.\n",
       "\n",
       "```julia-repl\n",
       "julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\n",
       "2×4 DataFrame\n",
       " Row │ x      y     z        w\n",
       "     │ Int64  Char  Float64  String\n",
       "─────┼──────────────────────────────\n",
       "   1 │     1  a        10.0  A\n",
       "   2 │     2  b        20.0  B\n",
       "\n",
       "julia> Z, XY, W = unpack(table, ==(:z), !=(:w));\n",
       "julia> Z\n",
       "2-element Vector{Float64}:\n",
       " 10.0\n",
       " 20.0\n",
       "\n",
       "julia> XY\n",
       "2×2 DataFrame\n",
       " Row │ x      y\n",
       "     │ Int64  Char\n",
       "─────┼─────────────\n",
       "   1 │     1  a\n",
       "   2 │     2  b\n",
       "\n",
       "julia> W  # the column(s) left over\n",
       "2-element Vector{String}:\n",
       " \"A\"\n",
       " \"B\"\n",
       "```\n",
       "\n",
       "Whenever a returned table contains a single column, it is converted to a vector unless `wrap_singles=true`.\n",
       "\n",
       "If `coerce_options` are specified then `table` is first replaced with `coerce(table, coerce_options)`. See [`ScientificTypes.coerce`](@ref) for details.\n",
       "\n",
       "If `shuffle=true` then the rows of `table` are first shuffled, using the global RNG, unless `rng` is specified; if `rng` is an integer, it specifies the seed of an automatically generated Mersenne twister. If `rng` is specified then `shuffle=true` is implicit.\n"
      ],
      "text/plain": [
       "\u001b[36m  unpack(table, f1, f2, ... fk;\u001b[39m\n",
       "\u001b[36m         wrap_singles=false,\u001b[39m\n",
       "\u001b[36m         shuffle=false,\u001b[39m\n",
       "\u001b[36m         rng::Union{AbstractRNG,Int,Nothing}=nothing,\u001b[39m\n",
       "\u001b[36m         coerce_options...)\u001b[39m\n",
       "\n",
       "  Horizontally split any Tables.jl compatible \u001b[36mtable\u001b[39m into smaller tables or\n",
       "  vectors by making column selections determined by the predicates \u001b[36mf1\u001b[39m, \u001b[36mf2\u001b[39m,\n",
       "  ..., \u001b[36mfk\u001b[39m. Selection from the column names is without replacement. A \u001b[4mpredicate\u001b[24m\n",
       "  is any object \u001b[36mf\u001b[39m such that \u001b[36mf(name)\u001b[39m is \u001b[36mtrue\u001b[39m or \u001b[36mfalse\u001b[39m for each column\n",
       "  \u001b[36mname::Symbol\u001b[39m of \u001b[36mtable\u001b[39m.\n",
       "\n",
       "  Returns a tuple of tables/vectors with length one greater than the number of\n",
       "  supplied predicates, with the last component including all previously\n",
       "  unselected columns.\n",
       "\n",
       "\u001b[36m  julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\u001b[39m\n",
       "\u001b[36m  2×4 DataFrame\u001b[39m\n",
       "\u001b[36m   Row │ x      y     z        w\u001b[39m\n",
       "\u001b[36m       │ Int64  Char  Float64  String\u001b[39m\n",
       "\u001b[36m  ─────┼──────────────────────────────\u001b[39m\n",
       "\u001b[36m     1 │     1  a        10.0  A\u001b[39m\n",
       "\u001b[36m     2 │     2  b        20.0  B\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> Z, XY, W = unpack(table, ==(:z), !=(:w));\u001b[39m\n",
       "\u001b[36m  julia> Z\u001b[39m\n",
       "\u001b[36m  2-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m   10.0\u001b[39m\n",
       "\u001b[36m   20.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> XY\u001b[39m\n",
       "\u001b[36m  2×2 DataFrame\u001b[39m\n",
       "\u001b[36m   Row │ x      y\u001b[39m\n",
       "\u001b[36m       │ Int64  Char\u001b[39m\n",
       "\u001b[36m  ─────┼─────────────\u001b[39m\n",
       "\u001b[36m     1 │     1  a\u001b[39m\n",
       "\u001b[36m     2 │     2  b\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> W  # the column(s) left over\u001b[39m\n",
       "\u001b[36m  2-element Vector{String}:\u001b[39m\n",
       "\u001b[36m   \"A\"\u001b[39m\n",
       "\u001b[36m   \"B\"\u001b[39m\n",
       "\n",
       "  Whenever a returned table contains a single column, it is converted to a\n",
       "  vector unless \u001b[36mwrap_singles=true\u001b[39m.\n",
       "\n",
       "  If \u001b[36mcoerce_options\u001b[39m are specified then \u001b[36mtable\u001b[39m is first replaced with\n",
       "  \u001b[36mcoerce(table, coerce_options)\u001b[39m. See \u001b[36mScientificTypes.coerce\u001b[39m for details.\n",
       "\n",
       "  If \u001b[36mshuffle=true\u001b[39m then the rows of \u001b[36mtable\u001b[39m are first shuffled, using the global\n",
       "  RNG, unless \u001b[36mrng\u001b[39m is specified; if \u001b[36mrng\u001b[39m is an integer, it specifies the seed of\n",
       "  an automatically generated Mersenne twister. If \u001b[36mrng\u001b[39m is specified then\n",
       "  \u001b[36mshuffle=true\u001b[39m is implicit."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Documentação da função\n",
    "@doc unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :constructor, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :target_in_fit, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n",
       " (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n",
       " (name = ARDRegressor, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = AdaBoostClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = AdaBoostRegressor, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = AffinityPropagation, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = AgglomerativeClustering, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = AutoEncoder, package_name = BetaML, ... )\n",
       " (name = BM25Transformer, package_name = MLJText, ... )\n",
       " (name = BaggingClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = BaggingRegressor, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = BayesianLDA, package_name = MLJScikitLearnInterface, ... )\n",
       " ⋮\n",
       " (name = TSVDTransformer, package_name = TSVD, ... )\n",
       " (name = TfidfTransformer, package_name = MLJText, ... )\n",
       " (name = TheilSenRegressor, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = TomekUndersampler, package_name = Imbalance, ... )\n",
       " (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n",
       " (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateFillImputer, package_name = MLJModels, ... )\n",
       " (name = UnivariateStandardizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )\n",
       " (name = XGBoostCount, package_name = XGBoost, ... )\n",
       " (name = XGBoostRegressor, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pesquisa por todos os modelos de ML disponíveis\n",
    "all_models = models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :constructor, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :target_in_fit, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = EvoLinearRegressor, package_name = EvoLinear, ... )\n",
       " (name = EvoSplineRegressor, package_name = EvoLinear, ... )\n",
       " (name = LinearBinaryClassifier, package_name = GLM, ... )\n",
       " (name = LinearCountRegressor, package_name = GLM, ... )\n",
       " (name = LinearRegressor, package_name = GLM, ... )\n",
       " (name = LinearRegressor, package_name = MLJLinearModels, ... )\n",
       " (name = LinearRegressor, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = LinearRegressor, package_name = MultivariateStats, ... )\n",
       " (name = MultitargetLinearRegressor, package_name = MultivariateStats, ... )\n",
       " (name = MultitargetRidgeRegressor, package_name = MultivariateStats, ... )\n",
       " (name = RidgeRegressor, package_name = MultivariateStats, ... )\n",
       " (name = SVMLinearRegressor, package_name = MLJScikitLearnInterface, ... )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pesquisa por todos os modelos de ML disponíveis, sendo do tipo Regressão Linear\n",
    "some_models = models(\"LinearRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(name = \"EvoLinearRegressor\",\n",
       " package_name = \"EvoLinear\",\n",
       " is_supervised = true,\n",
       " abstract_type = Deterministic,\n",
       " constructor = nothing,\n",
       " deep_properties = (),\n",
       " docstring = \"```\\nEvoLinearRegressor(; kwargs...)\\n```\\n\\nA model t...\",\n",
       " fit_data_scitype =\n",
       "     Tuple{Union{Table{<:Union{AbstractVector{<:Continuous}, AbstractVector{<:Count}, AbstractVector{<:OrderedFactor}}}, AbstractMatrix{Continuous}}, AbstractVector{<:Continuous}},\n",
       " human_name = \"evo linear regressor\",\n",
       " hyperparameter_ranges =\n",
       "     (nothing, nothing, nothing, nothing, nothing, nothing, nothing),\n",
       " hyperparameter_types =\n",
       "     (\"Symbol\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Any\", \"Symbol\"),\n",
       " hyperparameters = (:updater, :nrounds, :eta, :L1, :L2, :rng, :device),\n",
       " implemented_methods = [:fit, :predict, :update],\n",
       " inverse_transform_scitype = Unknown,\n",
       " is_pure_julia = true,\n",
       " is_wrapper = false,\n",
       " iteration_parameter = :nrounds,\n",
       " load_path = \"EvoLinear.EvoLinearRegressor\",\n",
       " package_license = \"MIT\",\n",
       " package_url = \"https://github.com/jeremiedb/EvoLinear.jl\",\n",
       " package_uuid = \"ab853011-1780-437f-b4b5-5de6f4777246\",\n",
       " predict_scitype = AbstractVector{<:Continuous},\n",
       " prediction_type = :deterministic,\n",
       " reporting_operations = (),\n",
       " reports_feature_importances = false,\n",
       " supports_class_weights = false,\n",
       " supports_online = false,\n",
       " supports_training_losses = false,\n",
       " supports_weights = false,\n",
       " target_in_fit = true,\n",
       " transform_scitype = Unknown,\n",
       " input_scitype =\n",
       "     Union{Table{<:Union{AbstractVector{<:Continuous}, AbstractVector{<:Count}, AbstractVector{<:OrderedFactor}}}, AbstractMatrix{Continuous}},\n",
       " target_scitype = AbstractVector{<:Continuous},\n",
       " output_scitype = Unknown)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza os detalhes de um dos modelos\n",
    "meta = some_models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AbstractVector{<:Continuous}\u001b[90m (alias for \u001b[39m\u001b[90mAbstractArray{<:Continuous, 1}\u001b[39m\u001b[90m)\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tipo de variável alvo do modelo anterior\n",
    "targetscitype = meta.target_scitype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica se podemos usar o modelo anterior com a nossa variável alvo\n",
    "# Não podemos, pois o modelo é de regressão e nossa variável é para classificação\n",
    "scitype(y) <: targetscitype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter_julia_classifiers (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função para filtrar modelos de classificação\n",
    "filter_julia_classifiers(meta) = AbstractVector{Finite} <: meta.target_scitype && meta.is_pure_julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :constructor, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :target_in_fit, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n",
       " (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = BetaML, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = OneRuleClassifier, package_name = OneRule, ... )\n",
       " (name = PegasosClassifier, package_name = BetaML, ... )\n",
       " (name = PerceptronClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = StableForestClassifier, package_name = SIRUS, ... )\n",
       " (name = StableRulesClassifier, package_name = SIRUS, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtra os modelos de classificação\n",
    "models(filter_julia_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :constructor, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :target_in_fit, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = AdaBoostClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = BayesianLDA, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianQDA, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = CatBoostClassifier, package_name = CatBoost, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " ⋮\n",
       " (name = RandomForestClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = RidgeCVClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = RidgeClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = SGDClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = SVMLinearClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = SVMNuClassifier, package_name = MLJScikitLearnInterface, ... )\n",
       " (name = StableForestClassifier, package_name = SIRUS, ... )\n",
       " (name = StableRulesClassifier, package_name = SIRUS, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quais modelos podem ser usados com nossas variáveis x e y?\n",
    "models(matching(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ZygoteRules ────────── v0.2.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FileIO ─────────────── v1.16.6\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m RealDot ────────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IRTools ────────────── v0.4.14\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Optimisers ─────────── v0.3.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JLD2 ───────────────── v0.5.9\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m GPUArrays ──────────── v10.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Functors ───────────── v0.4.12\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLDataDevices ──────── v1.5.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m OneHotArrays ───────── v0.2.6\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ProgressLogging ────── v0.1.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ChainRules ─────────── v1.72.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Zygote ─────────────── v0.6.73\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLJFlux ────────────── v0.6.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Metalhead ──────────── v0.9.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CommonSubexpressions ─ v0.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffRules ──────────── v1.15.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SparseInverseSubset ── v0.1.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PartialFunctions ───── v1.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffResults ────────── v1.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m AbstractFFTs ───────── v1.5.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StructArrays ───────── v0.6.18\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ForwardDiff ────────── v0.10.38\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Flux ───────────────── v0.14.25\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BSON ───────────────── v0.3.9\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\david\\Desktop\\Formacao_Engenheiro_inteligencia_artificial\\8-DS_e_ML_com_Julia\\JuliaLanguage\\env\\Project.toml`\n",
      "  \u001b[90m[094fc8d1] \u001b[39m\u001b[92m+ MLJFlux v0.6.0\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\david\\Desktop\\Formacao_Engenheiro_inteligencia_artificial\\8-DS_e_ML_com_Julia\\JuliaLanguage\\env\\Manifest.toml`\n",
      "  \u001b[90m[621f4979] \u001b[39m\u001b[92m+ AbstractFFTs v1.5.0\u001b[39m\n",
      "  \u001b[90m[fbb218c0] \u001b[39m\u001b[92m+ BSON v0.3.9\u001b[39m\n",
      "  \u001b[90m[082447d4] \u001b[39m\u001b[92m+ ChainRules v1.72.1\u001b[39m\n",
      "  \u001b[90m[bbf7d656] \u001b[39m\u001b[92m+ CommonSubexpressions v0.3.1\u001b[39m\n",
      "  \u001b[90m[163ba53b] \u001b[39m\u001b[92m+ DiffResults v1.1.0\u001b[39m\n",
      "  \u001b[90m[b552c78f] \u001b[39m\u001b[92m+ DiffRules v1.15.1\u001b[39m\n",
      "  \u001b[90m[5789e2e9] \u001b[39m\u001b[92m+ FileIO v1.16.6\u001b[39m\n",
      "  \u001b[90m[587475ba] \u001b[39m\u001b[92m+ Flux v0.14.25\u001b[39m\n",
      "  \u001b[90m[f6369f11] \u001b[39m\u001b[92m+ ForwardDiff v0.10.38\u001b[39m\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[d9f16b24] \u001b[39m\u001b[92m+ Functors v0.4.12\u001b[39m\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[0c68f7d7] \u001b[39m\u001b[92m+ GPUArrays v10.3.1\u001b[39m\n",
      "  \u001b[90m[7869d1d1] \u001b[39m\u001b[92m+ IRTools v0.4.14\u001b[39m\n",
      "  \u001b[90m[033835bb] \u001b[39m\u001b[92m+ JLD2 v0.5.9\u001b[39m\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[7e8f7934] \u001b[39m\u001b[92m+ MLDataDevices v1.5.3\u001b[39m\n",
      "  \u001b[90m[094fc8d1] \u001b[39m\u001b[92m+ MLJFlux v0.6.0\u001b[39m\n",
      "  \u001b[90m[dbeba491] \u001b[39m\u001b[92m+ Metalhead v0.9.4\u001b[39m\n",
      "  \u001b[90m[77ba4419] \u001b[39m\u001b[92m+ NaNMath v1.0.2\u001b[39m\n",
      "  \u001b[90m[0b1bfda6] \u001b[39m\u001b[92m+ OneHotArrays v0.2.6\u001b[39m\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[3bd65402] \u001b[39m\u001b[92m+ Optimisers v0.3.4\u001b[39m\n",
      "  \u001b[90m[570af359] \u001b[39m\u001b[92m+ PartialFunctions v1.2.0\u001b[39m\n",
      "  \u001b[90m[33c8b6b6] \u001b[39m\u001b[92m+ ProgressLogging v0.1.4\u001b[39m\n",
      "  \u001b[90m[c1ae055f] \u001b[39m\u001b[92m+ RealDot v0.1.0\u001b[39m\n",
      "  \u001b[90m[dc90abb0] \u001b[39m\u001b[92m+ SparseInverseSubset v0.1.2\u001b[39m\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[09ab397b] \u001b[39m\u001b[92m+ StructArrays v0.6.18\u001b[39m\n",
      "  \u001b[90m[e88e6eb3] \u001b[39m\u001b[92m+ Zygote v0.6.73\u001b[39m\n",
      "  \u001b[90m[700de1a5] \u001b[39m\u001b[92m+ ZygoteRules v0.2.5\u001b[39m\n",
      "\u001b[36m\u001b[1m        Info\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m and \u001b[33m⌅\u001b[39m have new versions available. Those with \u001b[32m⌃\u001b[39m may be upgradable, but those with \u001b[33m⌅\u001b[39m are restricted by compatibility constraints from upgrading. To see why use `status --outdated -m`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRealDot\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mProgressLogging\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAbstractFFTs\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCommonSubexpressions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFunctors\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBSON\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffResults\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSparseInverseSubset\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPartialFunctions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIRTools\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStructArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZygoteRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAbstractFFTs → AbstractFFTsChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFileIO\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGPUArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStructArrays → StructArraysAdaptExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAbstractFFTs → AbstractFFTsTestExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStructArrays → StructArraysSparseArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → AccessorsStructArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOptimisers\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStructArrays → StructArraysStaticArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStructArrays → StructArraysGPUArraysCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesFillArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFileIO → HTTPExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesSparseArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangStructArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesGPUArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mForwardDiff\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mForwardDiff → ForwardDiffStaticArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNNlib → NNlibForwardDiffExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mChainRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOneHotArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesChainRulesExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesMLUtilsExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesOneHotArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mJLD2\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZygote\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesZygoteExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZygote → ZygoteDistancesExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFlux\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMetalhead\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mMLJFlux\n",
      "  45 dependencies successfully precompiled in 98 seconds. 175 already precompiled.\n"
     ]
    }
   ],
   "source": [
    "# Instala o pacote MLJFlux\n",
    "Pkg.add(\"MLJFlux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJFlux ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJFlux.NeuralNetworkClassifier"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega o algoritmo de rede neural\n",
    "using MLJ\n",
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=\"MLJFlux\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkClassifier(\n",
       "  builder = Short(\n",
       "        n_hidden = 0, \n",
       "        dropout = 0.5, \n",
       "        σ = NNlib.σ), \n",
       "  finaliser = NNlib.softmax, \n",
       "  optimiser = Adam(0.001, (0.9, 0.999), 1.0e-8), \n",
       "  loss = Flux.Losses.crossentropy, \n",
       "  epochs = 10, \n",
       "  batch_size = 1, \n",
       "  lambda = 0.0, \n",
       "  alpha = 0.0, \n",
       "  rng = Random.TaskLocalRNG(), \n",
       "  optimiser_changes_trigger_retraining = false, \n",
       "  acceleration = CPU1{Nothing}(nothing), \n",
       "  embedding_dims = Dict{Symbol, Real}())"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria o modelo\n",
    "model = NeuralNetworkClassifier() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(name = \"NeuralNetworkClassifier\",\n",
       " package_name = \"MLJFlux\",\n",
       " is_supervised = true,\n",
       " abstract_type = Probabilistic,\n",
       " constructor = nothing,\n",
       " deep_properties = (:optimiser, :builder),\n",
       " docstring = \"```\\nNeuralNetworkClassifier\\n```\\n\\nA model type for ...\",\n",
       " fit_data_scitype =\n",
       "     Tuple{Union{Table{<:Union{AbstractVector{<:Continuous}, AbstractVector{<:Finite}}}, AbstractMatrix{Continuous}}, AbstractVector{<:Finite}},\n",
       " human_name = \"neural network classifier\",\n",
       " hyperparameter_ranges = (nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing),\n",
       " hyperparameter_types = (\"MLJFlux.Short\",\n",
       "                         \"typeof(NNlib.softmax)\",\n",
       "                         \"Optimisers.Adam\",\n",
       "                         \"typeof(Flux.Losses.crossentropy)\",\n",
       "                         \"Int64\",\n",
       "                         \"Int64\",\n",
       "                         \"Float64\",\n",
       "                         \"Float64\",\n",
       "                         \"Union{Int64, Random.AbstractRNG}\",\n",
       "                         \"Bool\",\n",
       "                         \"ComputationalResources.AbstractResource\",\n",
       "                         \"Dict{Symbol, Real}\"),\n",
       " hyperparameters = (:builder,\n",
       "                    :finaliser,\n",
       "                    :optimiser,\n",
       "                    :loss,\n",
       "                    :epochs,\n",
       "                    :batch_size,\n",
       "                    :lambda,\n",
       "                    :alpha,\n",
       "                    :rng,\n",
       "                    :optimiser_changes_trigger_retraining,\n",
       "                    :acceleration,\n",
       "                    :embedding_dims),\n",
       " implemented_methods = [],\n",
       " inverse_transform_scitype = Unknown,\n",
       " is_pure_julia = true,\n",
       " is_wrapper = false,\n",
       " iteration_parameter = :epochs,\n",
       " load_path = \"MLJFlux.NeuralNetworkClassifier\",\n",
       " package_license = \"MIT\",\n",
       " package_url = \"https://github.com/alan-turing-institute/MLJFlux.jl\",\n",
       " package_uuid = \"094fc8d1-fd35-5302-93ea-dabda2abf845\",\n",
       " predict_scitype =\n",
       "     AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:Finite},\n",
       " prediction_type = :probabilistic,\n",
       " reporting_operations = (),\n",
       " reports_feature_importances = false,\n",
       " supports_class_weights = false,\n",
       " supports_online = false,\n",
       " supports_training_losses = true,\n",
       " supports_weights = false,\n",
       " target_in_fit = true,\n",
       " transform_scitype = Unknown,\n",
       " input_scitype =\n",
       "     Union{Table{<:Union{AbstractVector{<:Continuous}, AbstractVector{<:Finite}}}, AbstractMatrix{Continuous}},\n",
       " target_scitype = AbstractVector{<:Finite},\n",
       " output_scitype = Unknown)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Info do modelo\n",
    "info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em MLJ um *modelo* é apenas uma estrutura contendo hiperparâmetros. Um modelo não armazena parâmetros *aprendidos* e os modelos são mutáveis. Para armazenar os parâmetros aprendidos usamos uma *machine*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número de epochs para treinar o modelo\n",
    "model.epochs = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica se o modelo está pronto para ser treinado em 30 epochs\n",
    "NeuralNetworkClassifier(epochs = 120) == model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "untrained Machine; caches model-specific representations of data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n",
       "    1:\tSource @626 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @654 ⏎ AbstractVector{Multiclass{3}}\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria o objeto que vai armazenar o modelo treinado (machine)\n",
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma máquina (mach) armazena parâmetros *aprendidos*, entre outras coisas. Treinamos esta máquina em 70% dos dados e avaliamos em 30% de dados de validação. Vamos começar dividindo todos os índices de linha em subconjuntos de `train` e `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  96, 97, 98, 99, 100, 101, 102, 103, 104, 105], [106, 107, 108, 109, 110, 111, 112, 113, 114, 115  …  141, 142, 143, 144, 145, 146, 147, 148, 149, 150])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide os dados em treino e teste\n",
    "train, test = partition(1:length(y), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …), …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMLJFlux: converting input data to Float32\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.381\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.185\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.21\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.119\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.146\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.097\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.055\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.084\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.07\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.055\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.028\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.029\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.074\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.037\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.022\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.024\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.05\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.025\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.029\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.004\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9885\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9894\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9644\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.025\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9747\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9422\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9608\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9411\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9507\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9322\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9421\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9431\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9378\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9302\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9176\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8972\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9065\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.884\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8689\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8613\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8502\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8842\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8727\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8405\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8688\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8261\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9075\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8569\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8729\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.856\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8171\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8031\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8471\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8516\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8416\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8241\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8848\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8447\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8448\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7974\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7565\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8306\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7783\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8143\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7793\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8318\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8155\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.743\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7257\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7554\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7516\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8004\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7455\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7692\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8315\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7161\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7549\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6952\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7424\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7925\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7238\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6931\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7134\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.674\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7609\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7008\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.8018\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7071\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6989\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6888\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6813\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7689\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6906\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7636\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6891\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7338\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6821\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6545\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6722\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7493\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6606\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6629\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7204\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7793\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6994\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7278\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6753\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.708\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6169\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.626\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7471\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6227\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6524\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6893\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.657\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.7192\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6591\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6111\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.6603\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n",
       "    1:\tSource @626 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @654 ⏎ AbstractVector{Multiclass{3}}\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinamento do modelo\n",
    "fit!(mach, rows = train, verbosity = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float32}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.119, Iris-versicolor=>0.518, Iris-virginica=>0.363)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0487, Iris-versicolor=>0.468, Iris-virginica=>0.483)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.889, Iris-versicolor=>0.108, Iris-virginica=>0.00284)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0115, Iris-versicolor=>0.402, Iris-virginica=>0.586)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.00762, Iris-versicolor=>0.375, Iris-virginica=>0.617)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.866, Iris-versicolor=>0.129, Iris-virginica=>0.00465)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0435, Iris-versicolor=>0.485, Iris-virginica=>0.471)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0638, Iris-versicolor=>0.515, Iris-virginica=>0.422)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0141, Iris-versicolor=>0.43, Iris-virginica=>0.556)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.151, Iris-versicolor=>0.524, Iris-virginica=>0.325)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0259, Iris-versicolor=>0.442, Iris-virginica=>0.532)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0846, Iris-versicolor=>0.503, Iris-virginica=>0.412)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0325, Iris-versicolor=>0.456, Iris-virginica=>0.512)\n",
       " ⋮\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0299, Iris-versicolor=>0.451, Iris-virginica=>0.519)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.897, Iris-versicolor=>0.101, Iris-virginica=>0.00245)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0148, Iris-versicolor=>0.414, Iris-virginica=>0.571)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.01, Iris-versicolor=>0.389, Iris-virginica=>0.601)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.02, Iris-versicolor=>0.422, Iris-virginica=>0.558)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.192, Iris-versicolor=>0.504, Iris-virginica=>0.304)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0108, Iris-versicolor=>0.4, Iris-virginica=>0.589)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.901, Iris-versicolor=>0.0968, Iris-virginica=>0.00225)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.00906, Iris-versicolor=>0.385, Iris-virginica=>0.606)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0172, Iris-versicolor=>0.421, Iris-virginica=>0.562)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.904, Iris-versicolor=>0.0937, Iris-virginica=>0.00204)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.905, Iris-versicolor=>0.0928, Iris-virginica=>0.002)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previsões com o modelo treinado usando dados de teste\n",
    "yhat = predict(mach, rows = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float32}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.119, Iris-versicolor=>0.518, Iris-virginica=>0.363)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0487, Iris-versicolor=>0.468, Iris-virginica=>0.483)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.889, Iris-versicolor=>0.108, Iris-virginica=>0.00284)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0115, Iris-versicolor=>0.402, Iris-virginica=>0.586)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.00762, Iris-versicolor=>0.375, Iris-virginica=>0.617)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza algumas previsões\n",
    "yhat[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chain = Chain(Chain(Dense(4 => 3, σ), Dropout(0.5), Dense(3 => 3)), softmax),)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estrutura do modelo\n",
    "fitted_params(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(training_losses = Float32[1.3292154, 1.3805449, 1.1850748, 1.2096461, 1.1187757, 1.1456356, 1.097053, 1.0552951, 1.084021, 1.0700724  …  0.7470699, 0.6227386, 0.65236354, 0.6892794, 0.65697426, 0.719236, 0.6591313, 0.6110671, 0.66033274, 0.65702176],)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relatório do erro durante o treinamento\n",
    "report(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45155316764201464"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcula o erro médio\n",
    "erro_medio = cross_entropy(predict(mach, X), y) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "setfield!: immutable struct of type Adam cannot be changed",
     "output_type": "error",
     "traceback": [
      "setfield!: immutable struct of type Adam cannot be changed",
      "",
      "Stacktrace:",
      " [1] setproperty!(x::Optimisers.Adam, f::Symbol, v::Float64)",
      "   @ Base .\\Base.jl:41",
      " [2] top-level scope",
      "   @ In[106]:2"
     ]
    }
   ],
   "source": [
    "# Alteramos um hiperparâmetro do modelo\n",
    "model.optimiser.eta = model.optimiser.eta * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "untrained Machine; caches model-specific representations of data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n",
       "    1:\tSource @661 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @010 ⏎ AbstractVector{Multiclass{3}}\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recriamos a machine\n",
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …), …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMLJFlux: converting input data to Float32\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.61\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.516\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.332\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.165\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.123\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.121\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.078\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.066\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.065\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.072\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.058\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.08\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.063\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.017\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.038\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.053\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.022\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9947\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.054\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.005\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.039\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9619\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.012\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9998\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.04\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 1.005\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9458\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9786\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9878\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss is 0.9382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n",
       "    1:\tSource @661 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @010 ⏎ AbstractVector{Multiclass{3}}\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinamos novamente o modelo\n",
    "fit!(mach, rows = train, verbosity = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8846421455129644"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erro médio do modelo\n",
    "erro_medio = cross_entropy(predict(mach, X), y) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericRange(1 ≤ epochs ≤ 200; origin=100.5, unit=99.5; on log10 scale)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Range de valores para criar a curva de aprendizado\n",
    "r = range(model, :epochs, lower = 1, upper = 200, scale = :log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(ProbabilisticTunedModel(model = NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …), …), …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mAttempting to evaluate 25 models.\n",
      "\u001b[33mEvaluating over 25 metamodels: 100%[=========================] Time: 0:00:04\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(parameter_name = \"epochs\",\n",
       " parameter_scale = :log10,\n",
       " parameter_values = [1, 2, 3, 4, 5, 6, 7, 9, 11, 13  …  39, 46, 56, 67, 80, 96, 116, 139, 167, 200],\n",
       " measurements = [1.026253769703873, 1.008868816889996, 0.9938698311607642, 0.9757023229117351, 0.9638335292563156, 0.9552255402899694, 0.9422079486229574, 0.9091892156790108, 0.8791728597076753, 0.8599942413711992  …  0.6531117054677827, 0.6228791495847761, 0.5944068118065242, 0.5462967708676447, 0.510526137066052, 0.46804746401089425, 0.42969617790960435, 0.407984844471726, 0.3737313072776364, 0.3518205214048733],)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Curva de aprendizado durante o treinamento\n",
    "curva_aprendizado = learning_curve(model, \n",
    "                                   X, \n",
    "                                   y,\n",
    "                                   range = r,\n",
    "                                   resampling = Holdout(fraction_train = 0.7),\n",
    "                                   measure = cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip940\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip941\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M219.866 1410.9 L2352.76 1410.9 L2352.76 47.2441 L219.866 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip942\">\n",
       "    <rect x=\"219\" y=\"47\" width=\"2134\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"280.231,1410.9 280.231,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1154.69,1410.9 1154.69,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2029.15,1410.9 2029.15,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1280.4 2352.76,1280.4 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1089.65 2352.76,1089.65 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,898.907 2352.76,898.907 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,708.159 2352.76,708.159 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,517.411 2352.76,517.411 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,326.664 2352.76,326.664 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,135.916 2352.76,135.916 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1410.9 2352.76,1410.9 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"280.231,1410.9 280.231,1392 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1154.69,1410.9 1154.69,1392 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2029.15,1410.9 2029.15,1392 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M245.214 1485.02 L252.853 1485.02 L252.853 1458.66 L244.543 1460.32 L244.543 1456.06 L252.806 1454.4 L257.482 1454.4 L257.482 1485.02 L265.121 1485.02 L265.121 1488.96 L245.214 1488.96 L245.214 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M284.566 1457.48 Q280.954 1457.48 279.126 1461.04 Q277.32 1464.58 277.32 1471.71 Q277.32 1478.82 279.126 1482.38 Q280.954 1485.92 284.566 1485.92 Q288.2 1485.92 290.005 1482.38 Q291.834 1478.82 291.834 1471.71 Q291.834 1464.58 290.005 1461.04 Q288.2 1457.48 284.566 1457.48 M284.566 1453.77 Q290.376 1453.77 293.431 1458.38 Q296.51 1462.96 296.51 1471.71 Q296.51 1480.44 293.431 1485.04 Q290.376 1489.63 284.566 1489.63 Q278.755 1489.63 275.677 1485.04 Q272.621 1480.44 272.621 1471.71 Q272.621 1462.96 275.677 1458.38 Q278.755 1453.77 284.566 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M306.215 1435.97 Q303.281 1435.97 301.795 1438.86 Q300.328 1441.74 300.328 1447.53 Q300.328 1453.31 301.795 1456.2 Q303.281 1459.08 306.215 1459.08 Q309.168 1459.08 310.635 1456.2 Q312.12 1453.31 312.12 1447.53 Q312.12 1441.74 310.635 1438.86 Q309.168 1435.97 306.215 1435.97 M306.215 1432.96 Q310.935 1432.96 313.418 1436.7 Q315.92 1440.43 315.92 1447.53 Q315.92 1454.62 313.418 1458.37 Q310.935 1462.09 306.215 1462.09 Q301.494 1462.09 298.993 1458.37 Q296.51 1454.62 296.51 1447.53 Q296.51 1440.43 298.993 1436.7 Q301.494 1432.96 306.215 1432.96 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1121.02 1485.02 L1128.66 1485.02 L1128.66 1458.66 L1120.35 1460.32 L1120.35 1456.06 L1128.61 1454.4 L1133.29 1454.4 L1133.29 1485.02 L1140.93 1485.02 L1140.93 1488.96 L1121.02 1488.96 L1121.02 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1160.37 1457.48 Q1156.76 1457.48 1154.93 1461.04 Q1153.13 1464.58 1153.13 1471.71 Q1153.13 1478.82 1154.93 1482.38 Q1156.76 1485.92 1160.37 1485.92 Q1164.01 1485.92 1165.81 1482.38 Q1167.64 1478.82 1167.64 1471.71 Q1167.64 1464.58 1165.81 1461.04 Q1164.01 1457.48 1160.37 1457.48 M1160.37 1453.77 Q1166.18 1453.77 1169.24 1458.38 Q1172.32 1462.96 1172.32 1471.71 Q1172.32 1480.44 1169.24 1485.04 Q1166.18 1489.63 1160.37 1489.63 Q1154.56 1489.63 1151.48 1485.04 Q1148.43 1480.44 1148.43 1471.71 Q1148.43 1462.96 1151.48 1458.38 Q1154.56 1453.77 1160.37 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1172.86 1458.35 L1179.07 1458.35 L1179.07 1436.93 L1172.32 1438.28 L1172.32 1434.82 L1179.03 1433.47 L1182.83 1433.47 L1182.83 1458.35 L1189.04 1458.35 L1189.04 1461.55 L1172.86 1461.55 L1172.86 1458.35 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1994.92 1485.02 L2002.56 1485.02 L2002.56 1458.66 L1994.25 1460.32 L1994.25 1456.06 L2002.52 1454.4 L2007.19 1454.4 L2007.19 1485.02 L2014.83 1485.02 L2014.83 1488.96 L1994.92 1488.96 L1994.92 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2034.28 1457.48 Q2030.67 1457.48 2028.84 1461.04 Q2027.03 1464.58 2027.03 1471.71 Q2027.03 1478.82 2028.84 1482.38 Q2030.67 1485.92 2034.28 1485.92 Q2037.91 1485.92 2039.72 1482.38 Q2041.55 1478.82 2041.55 1471.71 Q2041.55 1464.58 2039.72 1461.04 Q2037.91 1457.48 2034.28 1457.48 M2034.28 1453.77 Q2040.09 1453.77 2043.14 1458.38 Q2046.22 1462.96 2046.22 1471.71 Q2046.22 1480.44 2043.14 1485.04 Q2040.09 1489.63 2034.28 1489.63 Q2028.47 1489.63 2025.39 1485.04 Q2022.33 1480.44 2022.33 1471.71 Q2022.33 1462.96 2025.39 1458.38 Q2028.47 1453.77 2034.28 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2050.79 1458.35 L2064.05 1458.35 L2064.05 1461.55 L2046.22 1461.55 L2046.22 1458.35 Q2048.38 1456.11 2052.11 1452.35 Q2055.85 1448.57 2056.81 1447.48 Q2058.63 1445.43 2059.35 1444.02 Q2060.08 1442.59 2060.08 1441.22 Q2060.08 1438.98 2058.5 1437.57 Q2056.94 1436.16 2054.42 1436.16 Q2052.63 1436.16 2050.64 1436.78 Q2048.67 1437.4 2046.41 1438.66 L2046.41 1434.82 Q2048.7 1433.9 2050.7 1433.43 Q2052.69 1432.96 2054.35 1432.96 Q2058.71 1432.96 2061.3 1435.14 Q2063.9 1437.32 2063.9 1440.97 Q2063.9 1442.7 2063.24 1444.26 Q2062.6 1445.8 2060.89 1447.91 Q2060.42 1448.46 2057.9 1451.07 Q2055.38 1453.67 2050.79 1458.35 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1206.5 1554.9 L1206.5 1557.76 L1179.57 1557.76 Q1179.96 1563.81 1183.2 1566.99 Q1186.48 1570.14 1192.31 1570.14 Q1195.68 1570.14 1198.83 1569.32 Q1202.01 1568.49 1205.13 1566.83 L1205.13 1572.37 Q1201.98 1573.71 1198.67 1574.41 Q1195.36 1575.11 1191.96 1575.11 Q1183.43 1575.11 1178.43 1570.14 Q1173.46 1565.18 1173.46 1556.71 Q1173.46 1547.96 1178.17 1542.83 Q1182.92 1537.68 1190.94 1537.68 Q1198.13 1537.68 1202.3 1542.33 Q1206.5 1546.94 1206.5 1554.9 M1200.64 1553.18 Q1200.58 1548.37 1197.94 1545.51 Q1195.33 1542.64 1191 1542.64 Q1186.1 1542.64 1183.14 1545.41 Q1180.21 1548.18 1179.77 1553.21 L1200.64 1553.18 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1221.78 1568.84 L1221.78 1587.74 L1215.89 1587.74 L1215.89 1538.54 L1221.78 1538.54 L1221.78 1543.95 Q1223.62 1540.77 1226.43 1539.24 Q1229.26 1537.68 1233.17 1537.68 Q1239.67 1537.68 1243.71 1542.83 Q1247.78 1547.99 1247.78 1556.39 Q1247.78 1564.8 1243.71 1569.95 Q1239.67 1575.11 1233.17 1575.11 Q1229.26 1575.11 1226.43 1573.58 Q1223.62 1572.02 1221.78 1568.84 M1241.7 1556.39 Q1241.7 1549.93 1239.03 1546.27 Q1236.39 1542.58 1231.74 1542.58 Q1227.09 1542.58 1224.42 1546.27 Q1221.78 1549.93 1221.78 1556.39 Q1221.78 1562.85 1224.42 1566.55 Q1227.09 1570.21 1231.74 1570.21 Q1236.39 1570.21 1239.03 1566.55 Q1241.7 1562.85 1241.7 1556.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1271.3 1542.64 Q1266.59 1542.64 1263.86 1546.34 Q1261.12 1550 1261.12 1556.39 Q1261.12 1562.79 1263.82 1566.48 Q1266.56 1570.14 1271.3 1570.14 Q1275.98 1570.14 1278.72 1566.45 Q1281.46 1562.76 1281.46 1556.39 Q1281.46 1550.06 1278.72 1546.37 Q1275.98 1542.64 1271.3 1542.64 M1271.3 1537.68 Q1278.94 1537.68 1283.3 1542.64 Q1287.66 1547.61 1287.66 1556.39 Q1287.66 1565.15 1283.3 1570.14 Q1278.94 1575.11 1271.3 1575.11 Q1263.63 1575.11 1259.27 1570.14 Q1254.94 1565.15 1254.94 1556.39 Q1254.94 1547.61 1259.27 1542.64 Q1263.63 1537.68 1271.3 1537.68 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1323.03 1539.91 L1323.03 1545.38 Q1320.54 1544.01 1318.03 1543.34 Q1315.55 1542.64 1313 1542.64 Q1307.3 1542.64 1304.15 1546.27 Q1301 1549.87 1301 1556.39 Q1301 1562.92 1304.15 1566.55 Q1307.3 1570.14 1313 1570.14 Q1315.55 1570.14 1318.03 1569.47 Q1320.54 1568.77 1323.03 1567.41 L1323.03 1572.82 Q1320.57 1573.96 1317.93 1574.54 Q1315.32 1575.11 1312.36 1575.11 Q1304.31 1575.11 1299.57 1570.05 Q1294.83 1564.99 1294.83 1556.39 Q1294.83 1547.67 1299.6 1542.68 Q1304.41 1537.68 1312.74 1537.68 Q1315.45 1537.68 1318.03 1538.25 Q1320.61 1538.79 1323.03 1539.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1362.84 1552.67 L1362.84 1574.19 L1356.99 1574.19 L1356.99 1552.86 Q1356.99 1547.8 1355.01 1545.29 Q1353.04 1542.77 1349.09 1542.77 Q1344.35 1542.77 1341.61 1545.79 Q1338.88 1548.82 1338.88 1554.04 L1338.88 1574.19 L1332.99 1574.19 L1332.99 1524.66 L1338.88 1524.66 L1338.88 1544.08 Q1340.98 1540.86 1343.81 1539.27 Q1346.67 1537.68 1350.4 1537.68 Q1356.54 1537.68 1359.69 1541.5 Q1362.84 1545.29 1362.84 1552.67 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1397.25 1539.59 L1397.25 1545.13 Q1394.77 1543.85 1392.09 1543.22 Q1389.42 1542.58 1386.56 1542.58 Q1382.19 1542.58 1380 1543.92 Q1377.83 1545.25 1377.83 1547.93 Q1377.83 1549.96 1379.39 1551.14 Q1380.95 1552.29 1385.66 1553.34 L1387.67 1553.78 Q1393.91 1555.12 1396.52 1557.57 Q1399.16 1559.99 1399.16 1564.35 Q1399.16 1569.32 1395.21 1572.21 Q1391.3 1575.11 1384.42 1575.11 Q1381.56 1575.11 1378.44 1574.54 Q1375.35 1573.99 1371.91 1572.88 L1371.91 1566.83 Q1375.16 1568.52 1378.31 1569.38 Q1381.46 1570.21 1384.55 1570.21 Q1388.69 1570.21 1390.92 1568.81 Q1393.14 1567.37 1393.14 1564.8 Q1393.14 1562.41 1391.52 1561.14 Q1389.93 1559.86 1384.49 1558.68 L1382.45 1558.21 Q1377.01 1557.06 1374.59 1554.71 Q1372.17 1552.32 1372.17 1548.18 Q1372.17 1543.15 1375.73 1540.42 Q1379.3 1537.68 1385.85 1537.68 Q1389.1 1537.68 1391.97 1538.16 Q1394.83 1538.63 1397.25 1539.59 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1410.9 219.866,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1280.4 238.764,1280.4 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1089.65 238.764,1089.65 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,898.907 238.764,898.907 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,708.159 238.764,708.159 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,517.411 238.764,517.411 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,326.664 238.764,326.664 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,135.916 238.764,135.916 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M126.205 1266.2 Q122.593 1266.2 120.765 1269.77 Q118.959 1273.31 118.959 1280.44 Q118.959 1287.54 120.765 1291.11 Q122.593 1294.65 126.205 1294.65 Q129.839 1294.65 131.644 1291.11 Q133.473 1287.54 133.473 1280.44 Q133.473 1273.31 131.644 1269.77 Q129.839 1266.2 126.205 1266.2 M126.205 1262.5 Q132.015 1262.5 135.07 1267.1 Q138.149 1271.69 138.149 1280.44 Q138.149 1289.16 135.07 1293.77 Q132.015 1298.35 126.205 1298.35 Q120.394 1298.35 117.316 1293.77 Q114.26 1289.16 114.26 1280.44 Q114.26 1271.69 117.316 1267.1 Q120.394 1262.5 126.205 1262.5 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M146.366 1291.8 L151.251 1291.8 L151.251 1297.68 L146.366 1297.68 L146.366 1291.8 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M174.283 1267.2 L162.477 1285.64 L174.283 1285.64 L174.283 1267.2 M173.056 1263.12 L178.936 1263.12 L178.936 1285.64 L183.866 1285.64 L183.866 1289.53 L178.936 1289.53 L178.936 1297.68 L174.283 1297.68 L174.283 1289.53 L158.681 1289.53 L158.681 1285.02 L173.056 1263.12 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M127.686 1075.45 Q124.075 1075.45 122.246 1079.02 Q120.441 1082.56 120.441 1089.69 Q120.441 1096.8 122.246 1100.36 Q124.075 1103.9 127.686 1103.9 Q131.32 1103.9 133.126 1100.36 Q134.954 1096.8 134.954 1089.69 Q134.954 1082.56 133.126 1079.02 Q131.32 1075.45 127.686 1075.45 M127.686 1071.75 Q133.496 1071.75 136.552 1076.36 Q139.63 1080.94 139.63 1089.69 Q139.63 1098.42 136.552 1103.02 Q133.496 1107.61 127.686 1107.61 Q121.876 1107.61 118.797 1103.02 Q115.742 1098.42 115.742 1089.69 Q115.742 1080.94 118.797 1076.36 Q121.876 1071.75 127.686 1071.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M147.848 1101.05 L152.732 1101.05 L152.732 1106.93 L147.848 1106.93 L147.848 1101.05 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M162.964 1072.37 L181.32 1072.37 L181.32 1076.31 L167.246 1076.31 L167.246 1084.78 Q168.264 1084.43 169.283 1084.27 Q170.302 1084.09 171.32 1084.09 Q177.107 1084.09 180.487 1087.26 Q183.866 1090.43 183.866 1095.85 Q183.866 1101.42 180.394 1104.53 Q176.922 1107.61 170.602 1107.61 Q168.427 1107.61 166.158 1107.23 Q163.913 1106.86 161.505 1106.12 L161.505 1101.42 Q163.589 1102.56 165.811 1103.11 Q168.033 1103.67 170.51 1103.67 Q174.514 1103.67 176.852 1101.56 Q179.19 1099.46 179.19 1095.85 Q179.19 1092.24 176.852 1090.13 Q174.514 1088.02 170.51 1088.02 Q168.635 1088.02 166.76 1088.44 Q164.908 1088.86 162.964 1089.74 L162.964 1072.37 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M126.529 884.705 Q122.918 884.705 121.089 888.27 Q119.283 891.812 119.283 898.941 Q119.283 906.048 121.089 909.612 Q122.918 913.154 126.529 913.154 Q130.163 913.154 131.968 909.612 Q133.797 906.048 133.797 898.941 Q133.797 891.812 131.968 888.27 Q130.163 884.705 126.529 884.705 M126.529 881.002 Q132.339 881.002 135.394 885.608 Q138.473 890.191 138.473 898.941 Q138.473 907.668 135.394 912.274 Q132.339 916.858 126.529 916.858 Q120.718 916.858 117.64 912.274 Q114.584 907.668 114.584 898.941 Q114.584 890.191 117.64 885.608 Q120.718 881.002 126.529 881.002 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M146.691 910.307 L151.575 910.307 L151.575 916.187 L146.691 916.187 L146.691 910.307 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M172.339 897.043 Q169.19 897.043 167.339 899.196 Q165.51 901.349 165.51 905.099 Q165.51 908.825 167.339 911.001 Q169.19 913.154 172.339 913.154 Q175.487 913.154 177.315 911.001 Q179.167 908.825 179.167 905.099 Q179.167 901.349 177.315 899.196 Q175.487 897.043 172.339 897.043 M181.621 882.39 L181.621 886.65 Q179.862 885.816 178.056 885.376 Q176.274 884.937 174.514 884.937 Q169.885 884.937 167.431 888.062 Q165.001 891.187 164.653 897.506 Q166.019 895.492 168.079 894.427 Q170.139 893.339 172.616 893.339 Q177.825 893.339 180.834 896.511 Q183.866 899.659 183.866 905.099 Q183.866 910.423 180.718 913.64 Q177.57 916.858 172.339 916.858 Q166.343 916.858 163.172 912.274 Q160.001 907.668 160.001 898.941 Q160.001 890.747 163.89 885.886 Q167.778 881.002 174.329 881.002 Q176.089 881.002 177.871 881.349 Q179.676 881.696 181.621 882.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M127.593 693.958 Q123.982 693.958 122.154 697.522 Q120.348 701.064 120.348 708.194 Q120.348 715.3 122.154 718.865 Q123.982 722.407 127.593 722.407 Q131.228 722.407 133.033 718.865 Q134.862 715.3 134.862 708.194 Q134.862 701.064 133.033 697.522 Q131.228 693.958 127.593 693.958 M127.593 690.254 Q133.404 690.254 136.459 694.86 Q139.538 699.444 139.538 708.194 Q139.538 716.921 136.459 721.527 Q133.404 726.11 127.593 726.11 Q121.783 726.11 118.705 721.527 Q115.649 716.921 115.649 708.194 Q115.649 699.444 118.705 694.86 Q121.783 690.254 127.593 690.254 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M147.755 719.559 L152.64 719.559 L152.64 725.439 L147.755 725.439 L147.755 719.559 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M161.644 690.879 L183.866 690.879 L183.866 692.87 L171.32 725.439 L166.436 725.439 L178.241 694.814 L161.644 694.814 L161.644 690.879 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M126.783 503.21 Q123.172 503.21 121.343 506.775 Q119.538 510.317 119.538 517.446 Q119.538 524.553 121.343 528.117 Q123.172 531.659 126.783 531.659 Q130.417 531.659 132.223 528.117 Q134.052 524.553 134.052 517.446 Q134.052 510.317 132.223 506.775 Q130.417 503.21 126.783 503.21 M126.783 499.506 Q132.593 499.506 135.649 504.113 Q138.728 508.696 138.728 517.446 Q138.728 526.173 135.649 530.779 Q132.593 535.363 126.783 535.363 Q120.973 535.363 117.894 530.779 Q114.839 526.173 114.839 517.446 Q114.839 508.696 117.894 504.113 Q120.973 499.506 126.783 499.506 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M146.945 528.812 L151.829 528.812 L151.829 534.691 L146.945 534.691 L146.945 528.812 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M172.014 518.28 Q168.681 518.28 166.76 520.062 Q164.862 521.844 164.862 524.969 Q164.862 528.094 166.76 529.877 Q168.681 531.659 172.014 531.659 Q175.348 531.659 177.269 529.877 Q179.19 528.071 179.19 524.969 Q179.19 521.844 177.269 520.062 Q175.371 518.28 172.014 518.28 M167.339 516.289 Q164.329 515.548 162.64 513.488 Q160.973 511.428 160.973 508.465 Q160.973 504.321 163.913 501.914 Q166.876 499.506 172.014 499.506 Q177.176 499.506 180.116 501.914 Q183.056 504.321 183.056 508.465 Q183.056 511.428 181.366 513.488 Q179.7 515.548 176.714 516.289 Q180.093 517.076 181.968 519.367 Q183.866 521.659 183.866 524.969 Q183.866 529.992 180.788 532.678 Q177.732 535.363 172.014 535.363 Q166.297 535.363 163.218 532.678 Q160.163 529.992 160.163 524.969 Q160.163 521.659 162.061 519.367 Q163.959 517.076 167.339 516.289 M165.626 508.905 Q165.626 511.59 167.292 513.094 Q168.982 514.599 172.014 514.599 Q175.024 514.599 176.714 513.094 Q178.426 511.59 178.426 508.905 Q178.426 506.219 176.714 504.715 Q175.024 503.21 172.014 503.21 Q168.982 503.21 167.292 504.715 Q165.626 506.219 165.626 508.905 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M126.876 312.463 Q123.265 312.463 121.436 316.027 Q119.63 319.569 119.63 326.699 Q119.63 333.805 121.436 337.37 Q123.265 340.912 126.876 340.912 Q130.51 340.912 132.316 337.37 Q134.144 333.805 134.144 326.699 Q134.144 319.569 132.316 316.027 Q130.51 312.463 126.876 312.463 M126.876 308.759 Q132.686 308.759 135.742 313.365 Q138.82 317.949 138.82 326.699 Q138.82 335.425 135.742 340.032 Q132.686 344.615 126.876 344.615 Q121.066 344.615 117.987 340.032 Q114.931 335.425 114.931 326.699 Q114.931 317.949 117.987 313.365 Q121.066 308.759 126.876 308.759 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M147.038 338.064 L151.922 338.064 L151.922 343.944 L147.038 343.944 L147.038 338.064 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M162.246 343.226 L162.246 338.967 Q164.005 339.8 165.811 340.24 Q167.616 340.68 169.352 340.68 Q173.982 340.68 176.413 337.578 Q178.866 334.453 179.214 328.111 Q177.871 330.101 175.811 331.166 Q173.751 332.231 171.251 332.231 Q166.065 332.231 163.033 329.106 Q160.024 325.958 160.024 320.518 Q160.024 315.194 163.172 311.977 Q166.32 308.759 171.552 308.759 Q177.547 308.759 180.695 313.365 Q183.866 317.949 183.866 326.699 Q183.866 334.87 179.977 339.754 Q176.112 344.615 169.561 344.615 Q167.802 344.615 165.996 344.268 Q164.19 343.921 162.246 343.226 M171.552 328.574 Q174.7 328.574 176.528 326.421 Q178.38 324.268 178.38 320.518 Q178.38 316.791 176.528 314.639 Q174.7 312.463 171.552 312.463 Q168.403 312.463 166.552 314.639 Q164.723 316.791 164.723 320.518 Q164.723 324.268 166.552 326.421 Q168.403 328.574 171.552 328.574 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M117.501 149.261 L125.14 149.261 L125.14 122.896 L116.83 124.562 L116.83 120.303 L125.093 118.636 L129.769 118.636 L129.769 149.261 L137.408 149.261 L137.408 153.196 L117.501 153.196 L117.501 149.261 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M146.853 147.317 L151.737 147.317 L151.737 153.196 L146.853 153.196 L146.853 147.317 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M171.922 121.715 Q168.311 121.715 166.482 125.28 Q164.677 128.822 164.677 135.951 Q164.677 143.058 166.482 146.622 Q168.311 150.164 171.922 150.164 Q175.556 150.164 177.362 146.622 Q179.19 143.058 179.19 135.951 Q179.19 128.822 177.362 125.28 Q175.556 121.715 171.922 121.715 M171.922 118.011 Q177.732 118.011 180.788 122.618 Q183.866 127.201 183.866 135.951 Q183.866 144.678 180.788 149.284 Q177.732 153.868 171.922 153.868 Q166.112 153.868 163.033 149.284 Q159.978 144.678 159.978 135.951 Q159.978 127.201 163.033 122.618 Q166.112 118.011 171.922 118.011 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M16.4842 1228.95 L16.4842 1198.91 L21.895 1198.91 L21.895 1222.52 L35.9632 1222.52 L35.9632 1199.89 L41.3741 1199.89 L41.3741 1222.52 L58.5933 1222.52 L58.5933 1198.33 L64.0042 1198.33 L64.0042 1228.95 L16.4842 1228.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M33.8307 1167.37 Q33.2578 1168.35 33.0032 1169.53 Q32.7167 1170.68 32.7167 1172.08 Q32.7167 1177.04 35.9632 1179.71 Q39.1779 1182.36 45.2253 1182.36 L64.0042 1182.36 L64.0042 1188.24 L28.3562 1188.24 L28.3562 1182.36 L33.8944 1182.36 Q30.6479 1180.51 29.0883 1177.55 Q27.4968 1174.59 27.4968 1170.36 Q27.4968 1169.75 27.5923 1169.02 Q27.656 1168.29 27.8151 1167.4 L33.8307 1167.37 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M33.8307 1141.71 Q33.2578 1142.7 33.0032 1143.88 Q32.7167 1145.02 32.7167 1146.42 Q32.7167 1151.39 35.9632 1154.06 Q39.1779 1156.7 45.2253 1156.7 L64.0042 1156.7 L64.0042 1162.59 L28.3562 1162.59 L28.3562 1156.7 L33.8944 1156.7 Q30.6479 1154.86 29.0883 1151.9 Q27.4968 1148.94 27.4968 1144.7 Q27.4968 1144.1 27.5923 1143.37 Q27.656 1142.63 27.8151 1141.74 L33.8307 1141.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M32.4621 1123.19 Q32.4621 1127.9 36.1542 1130.64 Q39.8145 1133.37 46.212 1133.37 Q52.6095 1133.37 56.3017 1130.67 Q59.9619 1127.93 59.9619 1123.19 Q59.9619 1118.51 56.2698 1115.77 Q52.5777 1113.03 46.212 1113.03 Q39.8781 1113.03 36.186 1115.77 Q32.4621 1118.51 32.4621 1123.19 M27.4968 1123.19 Q27.4968 1115.55 32.4621 1111.19 Q37.4273 1106.83 46.212 1106.83 Q54.9649 1106.83 59.9619 1111.19 Q64.9272 1115.55 64.9272 1123.19 Q64.9272 1130.86 59.9619 1135.22 Q54.9649 1139.55 46.212 1139.55 Q37.4273 1139.55 32.4621 1135.22 Q27.4968 1130.86 27.4968 1123.19 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M41.3104 1069.72 L58.7206 1069.72 L58.7206 1059.4 Q58.7206 1054.21 56.5881 1051.73 Q54.4238 1049.22 49.9996 1049.22 Q45.5436 1049.22 43.4429 1051.73 Q41.3104 1054.21 41.3104 1059.4 L41.3104 1069.72 M21.7677 1069.72 L36.0905 1069.72 L36.0905 1060.2 Q36.0905 1055.49 34.34 1053.2 Q32.5576 1050.87 28.9291 1050.87 Q25.3325 1050.87 23.5501 1053.2 Q21.7677 1055.49 21.7677 1060.2 L21.7677 1069.72 M16.4842 1076.14 L16.4842 1059.72 Q16.4842 1052.37 19.5397 1048.39 Q22.5952 1044.41 28.2289 1044.41 Q32.5894 1044.41 35.1675 1046.45 Q37.7456 1048.49 38.3822 1052.43 Q39.4007 1047.69 42.6472 1045.08 Q45.8619 1042.44 50.6998 1042.44 Q57.0655 1042.44 60.5348 1046.77 Q64.0042 1051.1 64.0042 1059.08 L64.0042 1076.14 L16.4842 1076.14 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M46.0847 1015.48 Q46.0847 1022.58 47.7079 1025.31 Q49.3312 1028.05 53.2461 1028.05 Q56.3653 1028.05 58.2114 1026.01 Q60.0256 1023.95 60.0256 1020.41 Q60.0256 1015.54 56.5881 1012.61 Q53.1188 1009.65 47.3897 1009.65 L46.0847 1009.65 L46.0847 1015.48 M43.6657 1003.8 L64.0042 1003.8 L64.0042 1009.65 L58.5933 1009.65 Q61.8398 1011.66 63.3994 1014.65 Q64.9272 1017.64 64.9272 1021.97 Q64.9272 1027.45 61.8716 1030.69 Q58.7843 1033.91 53.6281 1033.91 Q47.6125 1033.91 44.5569 1029.9 Q41.5014 1025.86 41.5014 1017.87 L41.5014 1009.65 L40.9285 1009.65 Q36.8862 1009.65 34.6901 1012.33 Q32.4621 1014.97 32.4621 1019.78 Q32.4621 1022.83 33.1941 1025.73 Q33.9262 1028.62 35.3903 1031.3 L29.9795 1031.3 Q28.7381 1028.08 28.1334 1025.06 Q27.4968 1022.04 27.4968 1019.17 Q27.4968 1011.44 31.5072 1007.62 Q35.5176 1003.8 43.6657 1003.8 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M29.4065 969.01 L34.9447 969.01 Q33.6716 971.492 33.035 974.166 Q32.3984 976.84 32.3984 979.704 Q32.3984 984.065 33.7352 986.261 Q35.072 988.425 37.7456 988.425 Q39.7826 988.425 40.9603 986.866 Q42.1061 985.306 43.1565 980.595 L43.6021 978.59 Q44.9389 972.352 47.3897 969.742 Q49.8086 967.1 54.1691 967.1 Q59.1344 967.1 62.0308 971.047 Q64.9272 974.962 64.9272 981.837 Q64.9272 984.701 64.3543 987.82 Q63.8132 990.908 62.6992 994.345 L56.6518 994.345 Q58.3387 991.099 59.198 987.948 Q60.0256 984.797 60.0256 981.709 Q60.0256 977.572 58.6251 975.344 Q57.1929 973.116 54.6147 973.116 Q52.2276 973.116 50.9545 974.739 Q49.6813 976.33 48.5037 981.773 L48.0262 983.81 Q46.8804 989.253 44.5251 991.672 Q42.138 994.091 38.0002 994.091 Q32.9713 994.091 30.2341 990.526 Q27.4968 986.961 27.4968 980.404 Q27.4968 977.158 27.9743 974.293 Q28.4517 971.429 29.4065 969.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M44.7161 927.283 L47.5806 927.283 L47.5806 954.209 Q53.6281 953.828 56.8109 950.581 Q59.9619 947.303 59.9619 941.478 Q59.9619 938.104 59.1344 934.953 Q58.3069 931.77 56.6518 928.651 L62.1899 928.651 Q63.5267 931.802 64.227 935.112 Q64.9272 938.423 64.9272 941.828 Q64.9272 950.358 59.9619 955.355 Q54.9967 960.321 46.5303 960.321 Q37.7774 960.321 32.6531 955.61 Q27.4968 950.868 27.4968 942.847 Q27.4968 935.653 32.1438 931.484 Q36.7589 927.283 44.7161 927.283 M42.9973 933.139 Q38.1912 933.203 35.3266 935.844 Q32.4621 938.454 32.4621 942.783 Q32.4621 947.685 35.2312 950.645 Q38.0002 953.573 43.0292 954.019 L42.9973 933.139 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M46.0847 901.47 Q46.0847 908.567 47.7079 911.305 Q49.3312 914.042 53.2461 914.042 Q56.3653 914.042 58.2114 912.005 Q60.0256 909.936 60.0256 906.403 Q60.0256 901.533 56.5881 898.605 Q53.1188 895.645 47.3897 895.645 L46.0847 895.645 L46.0847 901.47 M43.6657 889.789 L64.0042 889.789 L64.0042 895.645 L58.5933 895.645 Q61.8398 897.65 63.3994 900.642 Q64.9272 903.634 64.9272 907.963 Q64.9272 913.437 61.8716 916.684 Q58.7843 919.898 53.6281 919.898 Q47.6125 919.898 44.5569 915.888 Q41.5014 911.846 41.5014 903.857 L41.5014 895.645 L40.9285 895.645 Q36.8862 895.645 34.6901 898.319 Q32.4621 900.96 32.4621 905.766 Q32.4621 908.822 33.1941 911.718 Q33.9262 914.615 35.3903 917.288 L29.9795 917.288 Q28.7381 914.074 28.1334 911.05 Q27.4968 908.026 27.4968 905.162 Q27.4968 897.427 31.5072 893.608 Q35.5176 889.789 43.6657 889.789 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M33.7671 854.268 L14.479 854.268 L14.479 848.411 L64.0042 848.411 L64.0042 854.268 L58.657 854.268 Q61.8398 856.114 63.3994 858.947 Q64.9272 861.748 64.9272 865.694 Q64.9272 872.156 59.771 876.23 Q54.6147 880.272 46.212 880.272 Q37.8093 880.272 32.6531 876.23 Q27.4968 872.156 27.4968 865.694 Q27.4968 861.748 29.0564 858.947 Q30.5842 856.114 33.7671 854.268 M46.212 874.224 Q52.6732 874.224 56.3653 871.583 Q60.0256 868.909 60.0256 864.262 Q60.0256 859.615 56.3653 856.942 Q52.6732 854.268 46.212 854.268 Q39.7508 854.268 36.0905 856.942 Q32.3984 859.615 32.3984 864.262 Q32.3984 868.909 36.0905 871.583 Q39.7508 874.224 46.212 874.224 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M32.4621 822.535 Q32.4621 827.245 36.1542 829.983 Q39.8145 832.72 46.212 832.72 Q52.6095 832.72 56.3017 830.015 Q59.9619 827.277 59.9619 822.535 Q59.9619 817.856 56.2698 815.119 Q52.5777 812.382 46.212 812.382 Q39.8781 812.382 36.186 815.119 Q32.4621 817.856 32.4621 822.535 M27.4968 822.535 Q27.4968 814.896 32.4621 810.535 Q37.4273 806.175 46.212 806.175 Q54.9649 806.175 59.9619 810.535 Q64.9272 814.896 64.9272 822.535 Q64.9272 830.206 59.9619 834.566 Q54.9649 838.895 46.212 838.895 Q37.4273 838.895 32.4621 834.566 Q27.4968 830.206 27.4968 822.535 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M42.4881 746.115 L64.0042 746.115 L64.0042 751.971 L42.679 751.971 Q37.6183 751.971 35.1038 753.944 Q32.5894 755.918 32.5894 759.864 Q32.5894 764.607 35.6131 767.344 Q38.6368 770.081 43.8567 770.081 L64.0042 770.081 L64.0042 775.97 L28.3562 775.97 L28.3562 770.081 L33.8944 770.081 Q30.6797 767.981 29.0883 765.148 Q27.4968 762.283 27.4968 758.56 Q27.4968 752.417 31.3163 749.266 Q35.1038 746.115 42.4881 746.115 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M46.0847 718.233 Q46.0847 725.331 47.7079 728.068 Q49.3312 730.805 53.2461 730.805 Q56.3653 730.805 58.2114 728.768 Q60.0256 726.699 60.0256 723.166 Q60.0256 718.296 56.5881 715.368 Q53.1188 712.408 47.3897 712.408 L46.0847 712.408 L46.0847 718.233 M43.6657 706.552 L64.0042 706.552 L64.0042 712.408 L58.5933 712.408 Q61.8398 714.413 63.3994 717.405 Q64.9272 720.397 64.9272 724.726 Q64.9272 730.2 61.8716 733.447 Q58.7843 736.661 53.6281 736.661 Q47.6125 736.661 44.5569 732.651 Q41.5014 728.609 41.5014 720.62 L41.5014 712.408 L40.9285 712.408 Q36.8862 712.408 34.6901 715.082 Q32.4621 717.723 32.4621 722.53 Q32.4621 725.585 33.1941 728.482 Q33.9262 731.378 35.3903 734.052 L29.9795 734.052 Q28.7381 730.837 28.1334 727.813 Q27.4968 724.789 27.4968 721.925 Q27.4968 714.191 31.5072 710.371 Q35.5176 706.552 43.6657 706.552 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M20.1444 637.929 L26.9239 637.929 Q23.9002 641.176 22.4043 644.868 Q20.9083 648.528 20.9083 652.666 Q20.9083 660.814 25.9054 665.143 Q30.8707 669.471 40.2919 669.471 Q49.6813 669.471 54.6784 665.143 Q59.6436 660.814 59.6436 652.666 Q59.6436 648.528 58.1477 644.868 Q56.6518 641.176 53.6281 637.929 L60.3439 637.929 Q62.6355 641.303 63.7814 645.091 Q64.9272 648.847 64.9272 653.048 Q64.9272 663.838 58.3387 670.044 Q51.7183 676.251 40.2919 676.251 Q28.8336 676.251 22.2451 670.044 Q15.6248 663.838 15.6248 653.048 Q15.6248 648.783 16.7706 645.027 Q17.8846 641.24 20.1444 637.929 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M33.8307 607.597 Q33.2578 608.583 33.0032 609.761 Q32.7167 610.907 32.7167 612.307 Q32.7167 617.273 35.9632 619.946 Q39.1779 622.588 45.2253 622.588 L64.0042 622.588 L64.0042 628.476 L28.3562 628.476 L28.3562 622.588 L33.8944 622.588 Q30.6479 620.742 29.0883 617.782 Q27.4968 614.822 27.4968 610.589 Q27.4968 609.984 27.5923 609.252 Q27.656 608.52 27.8151 607.629 L33.8307 607.597 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M32.4621 589.073 Q32.4621 593.783 36.1542 596.52 Q39.8145 599.258 46.212 599.258 Q52.6095 599.258 56.3017 596.552 Q59.9619 593.815 59.9619 589.073 Q59.9619 584.394 56.2698 581.657 Q52.5777 578.919 46.212 578.919 Q39.8781 578.919 36.186 581.657 Q32.4621 584.394 32.4621 589.073 M27.4968 589.073 Q27.4968 581.434 32.4621 577.073 Q37.4273 572.713 46.212 572.713 Q54.9649 572.713 59.9619 577.073 Q64.9272 581.434 64.9272 589.073 Q64.9272 596.743 59.9619 601.104 Q54.9649 605.432 46.212 605.432 Q37.4273 605.432 32.4621 601.104 Q27.4968 596.743 27.4968 589.073 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M29.4065 540.279 L34.9447 540.279 Q33.6716 542.762 33.035 545.436 Q32.3984 548.109 32.3984 550.974 Q32.3984 555.334 33.7352 557.53 Q35.072 559.695 37.7456 559.695 Q39.7826 559.695 40.9603 558.135 Q42.1061 556.576 43.1565 551.865 L43.6021 549.86 Q44.9389 543.621 47.3897 541.011 Q49.8086 538.37 54.1691 538.37 Q59.1344 538.37 62.0308 542.316 Q64.9272 546.231 64.9272 553.106 Q64.9272 555.971 64.3543 559.09 Q63.8132 562.177 62.6992 565.615 L56.6518 565.615 Q58.3387 562.368 59.198 559.217 Q60.0256 556.066 60.0256 552.979 Q60.0256 548.841 58.6251 546.613 Q57.1929 544.385 54.6147 544.385 Q52.2276 544.385 50.9545 546.009 Q49.6813 547.6 48.5037 553.043 L48.0262 555.08 Q46.8804 560.522 44.5251 562.941 Q42.138 565.36 38.0002 565.36 Q32.9713 565.36 30.2341 561.796 Q27.4968 558.231 27.4968 551.674 Q27.4968 548.428 27.9743 545.563 Q28.4517 542.698 29.4065 540.279 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M29.4065 506.318 L34.9447 506.318 Q33.6716 508.801 33.035 511.475 Q32.3984 514.148 32.3984 517.013 Q32.3984 521.373 33.7352 523.569 Q35.072 525.734 37.7456 525.734 Q39.7826 525.734 40.9603 524.174 Q42.1061 522.615 43.1565 517.904 L43.6021 515.899 Q44.9389 509.66 47.3897 507.05 Q49.8086 504.409 54.1691 504.409 Q59.1344 504.409 62.0308 508.355 Q64.9272 512.27 64.9272 519.145 Q64.9272 522.01 64.3543 525.129 Q63.8132 528.216 62.6992 531.654 L56.6518 531.654 Q58.3387 528.407 59.198 525.256 Q60.0256 522.105 60.0256 519.018 Q60.0256 514.88 58.6251 512.652 Q57.1929 510.424 54.6147 510.424 Q52.2276 510.424 50.9545 512.048 Q49.6813 513.639 48.5037 519.082 L48.0262 521.119 Q46.8804 526.561 44.5251 528.98 Q42.138 531.399 38.0002 531.399 Q32.9713 531.399 30.2341 527.834 Q27.4968 524.27 27.4968 517.713 Q27.4968 514.466 27.9743 511.602 Q28.4517 508.737 29.4065 506.318 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M16.4842 474.108 L16.4842 444.062 L21.895 444.062 L21.895 467.679 L35.9632 467.679 L35.9632 445.048 L41.3741 445.048 L41.3741 467.679 L58.5933 467.679 L58.5933 443.489 L64.0042 443.489 L64.0042 474.108 L16.4842 474.108 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M42.4881 403.544 L64.0042 403.544 L64.0042 409.4 L42.679 409.4 Q37.6183 409.4 35.1038 411.374 Q32.5894 413.347 32.5894 417.294 Q32.5894 422.036 35.6131 424.774 Q38.6368 427.511 43.8567 427.511 L64.0042 427.511 L64.0042 433.399 L28.3562 433.399 L28.3562 427.511 L33.8944 427.511 Q30.6797 425.41 29.0883 422.578 Q27.4968 419.713 27.4968 415.989 Q27.4968 409.846 31.3163 406.695 Q35.1038 403.544 42.4881 403.544 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M18.2347 386.07 L28.3562 386.07 L28.3562 374.007 L32.9077 374.007 L32.9077 386.07 L52.2594 386.07 Q56.6199 386.07 57.8613 384.893 Q59.1026 383.683 59.1026 380.023 L59.1026 374.007 L64.0042 374.007 L64.0042 380.023 Q64.0042 386.802 61.4897 389.38 Q58.9434 391.958 52.2594 391.958 L32.9077 391.958 L32.9077 396.255 L28.3562 396.255 L28.3562 391.958 L18.2347 391.958 L18.2347 386.07 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M33.8307 345.648 Q33.2578 346.635 33.0032 347.812 Q32.7167 348.958 32.7167 350.359 Q32.7167 355.324 35.9632 357.997 Q39.1779 360.639 45.2253 360.639 L64.0042 360.639 L64.0042 366.527 L28.3562 366.527 L28.3562 360.639 L33.8944 360.639 Q30.6479 358.793 29.0883 355.833 Q27.4968 352.873 27.4968 348.64 Q27.4968 348.035 27.5923 347.303 Q27.656 346.571 27.8151 345.68 L33.8307 345.648 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M32.4621 327.124 Q32.4621 331.834 36.1542 334.572 Q39.8145 337.309 46.212 337.309 Q52.6095 337.309 56.3017 334.603 Q59.9619 331.866 59.9619 327.124 Q59.9619 322.445 56.2698 319.708 Q52.5777 316.97 46.212 316.97 Q39.8781 316.97 36.186 319.708 Q32.4621 322.445 32.4621 327.124 M27.4968 327.124 Q27.4968 319.485 32.4621 315.124 Q37.4273 310.764 46.212 310.764 Q54.9649 310.764 59.9619 315.124 Q64.9272 319.485 64.9272 327.124 Q64.9272 334.794 59.9619 339.155 Q54.9649 343.484 46.212 343.484 Q37.4273 343.484 32.4621 339.155 Q27.4968 334.794 27.4968 327.124 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M58.657 295.391 L77.5631 295.391 L77.5631 301.279 L28.3562 301.279 L28.3562 295.391 L33.7671 295.391 Q30.5842 293.545 29.0564 290.744 Q27.4968 287.911 27.4968 283.996 Q27.4968 277.503 32.6531 273.461 Q37.8093 269.387 46.212 269.387 Q54.6147 269.387 59.771 273.461 Q64.9272 277.503 64.9272 283.996 Q64.9272 287.911 63.3994 290.744 Q61.8398 293.545 58.657 295.391 M46.212 275.466 Q39.7508 275.466 36.0905 278.14 Q32.3984 280.781 32.3984 285.428 Q32.3984 290.075 36.0905 292.749 Q39.7508 295.391 46.212 295.391 Q52.6732 295.391 56.3653 292.749 Q60.0256 290.075 60.0256 285.428 Q60.0256 280.781 56.3653 278.14 Q52.6732 275.466 46.212 275.466 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M67.3143 244.847 Q73.68 247.33 75.6216 249.685 Q77.5631 252.04 77.5631 255.987 L77.5631 260.666 L72.6615 260.666 L72.6615 257.228 Q72.6615 254.809 71.5157 253.473 Q70.3699 252.136 66.1048 250.512 L63.4312 249.462 L28.3562 263.88 L28.3562 257.674 L56.238 246.534 L28.3562 235.394 L28.3562 229.187 L67.3143 244.847 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip942)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"280.231,85.838 543.47,118.999 697.455,147.61 806.709,182.264 891.453,204.903 960.694,221.323 1019.24,246.153 1114.68,309.136 1190.89,366.391 1254.33,402.974 1308.68,454.576 1398.45,526.719 1454.13,581.085 1531.9,649.462 1596.43,725.293 1671.55,797.597 1734.25,855.265 1808.95,909.575 1877.06,1001.34 1944.41,1069.58 2013.65,1150.6 2085.52,1223.76 2154.21,1265.17 2223.91,1330.51 2292.39,1372.3 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M2015.94 196.379 L2281.66 196.379 L2281.66 92.6992 L2015.94 92.6992  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2015.94,196.379 2281.66,196.379 2281.66,92.6992 2015.94,92.6992 2015.94,196.379 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2039.64,144.539 2181.83,144.539 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M2219.37 164.227 Q2217.57 168.856 2215.85 170.268 Q2214.14 171.68 2211.27 171.68 L2207.87 171.68 L2207.87 168.115 L2210.37 168.115 Q2212.13 168.115 2213.1 167.282 Q2214.07 166.449 2215.25 163.347 L2216.02 161.403 L2205.53 135.893 L2210.04 135.893 L2218.15 156.171 L2226.25 135.893 L2230.76 135.893 L2219.37 164.227 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2238.05 157.884 L2245.69 157.884 L2245.69 131.518 L2237.38 133.185 L2237.38 128.926 L2245.65 127.259 L2250.32 127.259 L2250.32 157.884 L2257.96 157.884 L2257.96 161.819 L2238.05 161.819 L2238.05 157.884 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip990\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip990)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip991\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip990)\" d=\"M219.866 1410.9 L2352.76 1410.9 L2352.76 47.2441 L219.866 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip992\">\n",
       "    <rect x=\"219\" y=\"47\" width=\"2134\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"280.231,1410.9 280.231,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1154.69,1410.9 1154.69,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2029.15,1410.9 2029.15,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1280.4 2352.76,1280.4 \"/>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1089.65 2352.76,1089.65 \"/>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,898.907 2352.76,898.907 \"/>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,708.159 2352.76,708.159 \"/>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,517.411 2352.76,517.411 \"/>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,326.664 2352.76,326.664 \"/>\n",
       "<polyline clip-path=\"url(#clip992)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,135.916 2352.76,135.916 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1410.9 2352.76,1410.9 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"280.231,1410.9 280.231,1392 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1154.69,1410.9 1154.69,1392 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2029.15,1410.9 2029.15,1392 \"/>\n",
       "<path clip-path=\"url(#clip990)\" d=\"M245.214 1485.02 L252.853 1485.02 L252.853 1458.66 L244.543 1460.32 L244.543 1456.06 L252.806 1454.4 L257.482 1454.4 L257.482 1485.02 L265.121 1485.02 L265.121 1488.96 L245.214 1488.96 L245.214 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M284.566 1457.48 Q280.954 1457.48 279.126 1461.04 Q277.32 1464.58 277.32 1471.71 Q277.32 1478.82 279.126 1482.38 Q280.954 1485.92 284.566 1485.92 Q288.2 1485.92 290.005 1482.38 Q291.834 1478.82 291.834 1471.71 Q291.834 1464.58 290.005 1461.04 Q288.2 1457.48 284.566 1457.48 M284.566 1453.77 Q290.376 1453.77 293.431 1458.38 Q296.51 1462.96 296.51 1471.71 Q296.51 1480.44 293.431 1485.04 Q290.376 1489.63 284.566 1489.63 Q278.755 1489.63 275.677 1485.04 Q272.621 1480.44 272.621 1471.71 Q272.621 1462.96 275.677 1458.38 Q278.755 1453.77 284.566 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M306.215 1435.97 Q303.281 1435.97 301.795 1438.86 Q300.328 1441.74 300.328 1447.53 Q300.328 1453.31 301.795 1456.2 Q303.281 1459.08 306.215 1459.08 Q309.168 1459.08 310.635 1456.2 Q312.12 1453.31 312.12 1447.53 Q312.12 1441.74 310.635 1438.86 Q309.168 1435.97 306.215 1435.97 M306.215 1432.96 Q310.935 1432.96 313.418 1436.7 Q315.92 1440.43 315.92 1447.53 Q315.92 1454.62 313.418 1458.37 Q310.935 1462.09 306.215 1462.09 Q301.494 1462.09 298.993 1458.37 Q296.51 1454.62 296.51 1447.53 Q296.51 1440.43 298.993 1436.7 Q301.494 1432.96 306.215 1432.96 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1121.02 1485.02 L1128.66 1485.02 L1128.66 1458.66 L1120.35 1460.32 L1120.35 1456.06 L1128.61 1454.4 L1133.29 1454.4 L1133.29 1485.02 L1140.93 1485.02 L1140.93 1488.96 L1121.02 1488.96 L1121.02 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1160.37 1457.48 Q1156.76 1457.48 1154.93 1461.04 Q1153.13 1464.58 1153.13 1471.71 Q1153.13 1478.82 1154.93 1482.38 Q1156.76 1485.92 1160.37 1485.92 Q1164.01 1485.92 1165.81 1482.38 Q1167.64 1478.82 1167.64 1471.71 Q1167.64 1464.58 1165.81 1461.04 Q1164.01 1457.48 1160.37 1457.48 M1160.37 1453.77 Q1166.18 1453.77 1169.24 1458.38 Q1172.32 1462.96 1172.32 1471.71 Q1172.32 1480.44 1169.24 1485.04 Q1166.18 1489.63 1160.37 1489.63 Q1154.56 1489.63 1151.48 1485.04 Q1148.43 1480.44 1148.43 1471.71 Q1148.43 1462.96 1151.48 1458.38 Q1154.56 1453.77 1160.37 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1172.86 1458.35 L1179.07 1458.35 L1179.07 1436.93 L1172.32 1438.28 L1172.32 1434.82 L1179.03 1433.47 L1182.83 1433.47 L1182.83 1458.35 L1189.04 1458.35 L1189.04 1461.55 L1172.86 1461.55 L1172.86 1458.35 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1994.92 1485.02 L2002.56 1485.02 L2002.56 1458.66 L1994.25 1460.32 L1994.25 1456.06 L2002.52 1454.4 L2007.19 1454.4 L2007.19 1485.02 L2014.83 1485.02 L2014.83 1488.96 L1994.92 1488.96 L1994.92 1485.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M2034.28 1457.48 Q2030.67 1457.48 2028.84 1461.04 Q2027.03 1464.58 2027.03 1471.71 Q2027.03 1478.82 2028.84 1482.38 Q2030.67 1485.92 2034.28 1485.92 Q2037.91 1485.92 2039.72 1482.38 Q2041.55 1478.82 2041.55 1471.71 Q2041.55 1464.58 2039.72 1461.04 Q2037.91 1457.48 2034.28 1457.48 M2034.28 1453.77 Q2040.09 1453.77 2043.14 1458.38 Q2046.22 1462.96 2046.22 1471.71 Q2046.22 1480.44 2043.14 1485.04 Q2040.09 1489.63 2034.28 1489.63 Q2028.47 1489.63 2025.39 1485.04 Q2022.33 1480.44 2022.33 1471.71 Q2022.33 1462.96 2025.39 1458.38 Q2028.47 1453.77 2034.28 1453.77 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M2050.79 1458.35 L2064.05 1458.35 L2064.05 1461.55 L2046.22 1461.55 L2046.22 1458.35 Q2048.38 1456.11 2052.11 1452.35 Q2055.85 1448.57 2056.81 1447.48 Q2058.63 1445.43 2059.35 1444.02 Q2060.08 1442.59 2060.08 1441.22 Q2060.08 1438.98 2058.5 1437.57 Q2056.94 1436.16 2054.42 1436.16 Q2052.63 1436.16 2050.64 1436.78 Q2048.67 1437.4 2046.41 1438.66 L2046.41 1434.82 Q2048.7 1433.9 2050.7 1433.43 Q2052.69 1432.96 2054.35 1432.96 Q2058.71 1432.96 2061.3 1435.14 Q2063.9 1437.32 2063.9 1440.97 Q2063.9 1442.7 2063.24 1444.26 Q2062.6 1445.8 2060.89 1447.91 Q2060.42 1448.46 2057.9 1451.07 Q2055.38 1453.67 2050.79 1458.35 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1206.5 1554.9 L1206.5 1557.76 L1179.57 1557.76 Q1179.96 1563.81 1183.2 1566.99 Q1186.48 1570.14 1192.31 1570.14 Q1195.68 1570.14 1198.83 1569.32 Q1202.01 1568.49 1205.13 1566.83 L1205.13 1572.37 Q1201.98 1573.71 1198.67 1574.41 Q1195.36 1575.11 1191.96 1575.11 Q1183.43 1575.11 1178.43 1570.14 Q1173.46 1565.18 1173.46 1556.71 Q1173.46 1547.96 1178.17 1542.83 Q1182.92 1537.68 1190.94 1537.68 Q1198.13 1537.68 1202.3 1542.33 Q1206.5 1546.94 1206.5 1554.9 M1200.64 1553.18 Q1200.58 1548.37 1197.94 1545.51 Q1195.33 1542.64 1191 1542.64 Q1186.1 1542.64 1183.14 1545.41 Q1180.21 1548.18 1179.77 1553.21 L1200.64 1553.18 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1221.78 1568.84 L1221.78 1587.74 L1215.89 1587.74 L1215.89 1538.54 L1221.78 1538.54 L1221.78 1543.95 Q1223.62 1540.77 1226.43 1539.24 Q1229.26 1537.68 1233.17 1537.68 Q1239.67 1537.68 1243.71 1542.83 Q1247.78 1547.99 1247.78 1556.39 Q1247.78 1564.8 1243.71 1569.95 Q1239.67 1575.11 1233.17 1575.11 Q1229.26 1575.11 1226.43 1573.58 Q1223.62 1572.02 1221.78 1568.84 M1241.7 1556.39 Q1241.7 1549.93 1239.03 1546.27 Q1236.39 1542.58 1231.74 1542.58 Q1227.09 1542.58 1224.42 1546.27 Q1221.78 1549.93 1221.78 1556.39 Q1221.78 1562.85 1224.42 1566.55 Q1227.09 1570.21 1231.74 1570.21 Q1236.39 1570.21 1239.03 1566.55 Q1241.7 1562.85 1241.7 1556.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1271.3 1542.64 Q1266.59 1542.64 1263.86 1546.34 Q1261.12 1550 1261.12 1556.39 Q1261.12 1562.79 1263.82 1566.48 Q1266.56 1570.14 1271.3 1570.14 Q1275.98 1570.14 1278.72 1566.45 Q1281.46 1562.76 1281.46 1556.39 Q1281.46 1550.06 1278.72 1546.37 Q1275.98 1542.64 1271.3 1542.64 M1271.3 1537.68 Q1278.94 1537.68 1283.3 1542.64 Q1287.66 1547.61 1287.66 1556.39 Q1287.66 1565.15 1283.3 1570.14 Q1278.94 1575.11 1271.3 1575.11 Q1263.63 1575.11 1259.27 1570.14 Q1254.94 1565.15 1254.94 1556.39 Q1254.94 1547.61 1259.27 1542.64 Q1263.63 1537.68 1271.3 1537.68 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1323.03 1539.91 L1323.03 1545.38 Q1320.54 1544.01 1318.03 1543.34 Q1315.55 1542.64 1313 1542.64 Q1307.3 1542.64 1304.15 1546.27 Q1301 1549.87 1301 1556.39 Q1301 1562.92 1304.15 1566.55 Q1307.3 1570.14 1313 1570.14 Q1315.55 1570.14 1318.03 1569.47 Q1320.54 1568.77 1323.03 1567.41 L1323.03 1572.82 Q1320.57 1573.96 1317.93 1574.54 Q1315.32 1575.11 1312.36 1575.11 Q1304.31 1575.11 1299.57 1570.05 Q1294.83 1564.99 1294.83 1556.39 Q1294.83 1547.67 1299.6 1542.68 Q1304.41 1537.68 1312.74 1537.68 Q1315.45 1537.68 1318.03 1538.25 Q1320.61 1538.79 1323.03 1539.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1362.84 1552.67 L1362.84 1574.19 L1356.99 1574.19 L1356.99 1552.86 Q1356.99 1547.8 1355.01 1545.29 Q1353.04 1542.77 1349.09 1542.77 Q1344.35 1542.77 1341.61 1545.79 Q1338.88 1548.82 1338.88 1554.04 L1338.88 1574.19 L1332.99 1574.19 L1332.99 1524.66 L1338.88 1524.66 L1338.88 1544.08 Q1340.98 1540.86 1343.81 1539.27 Q1346.67 1537.68 1350.4 1537.68 Q1356.54 1537.68 1359.69 1541.5 Q1362.84 1545.29 1362.84 1552.67 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M1397.25 1539.59 L1397.25 1545.13 Q1394.77 1543.85 1392.09 1543.22 Q1389.42 1542.58 1386.56 1542.58 Q1382.19 1542.58 1380 1543.92 Q1377.83 1545.25 1377.83 1547.93 Q1377.83 1549.96 1379.39 1551.14 Q1380.95 1552.29 1385.66 1553.34 L1387.67 1553.78 Q1393.91 1555.12 1396.52 1557.57 Q1399.16 1559.99 1399.16 1564.35 Q1399.16 1569.32 1395.21 1572.21 Q1391.3 1575.11 1384.42 1575.11 Q1381.56 1575.11 1378.44 1574.54 Q1375.35 1573.99 1371.91 1572.88 L1371.91 1566.83 Q1375.16 1568.52 1378.31 1569.38 Q1381.46 1570.21 1384.55 1570.21 Q1388.69 1570.21 1390.92 1568.81 Q1393.14 1567.37 1393.14 1564.8 Q1393.14 1562.41 1391.52 1561.14 Q1389.93 1559.86 1384.49 1558.68 L1382.45 1558.21 Q1377.01 1557.06 1374.59 1554.71 Q1372.17 1552.32 1372.17 1548.18 Q1372.17 1543.15 1375.73 1540.42 Q1379.3 1537.68 1385.85 1537.68 Q1389.1 1537.68 1391.97 1538.16 Q1394.83 1538.63 1397.25 1539.59 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1410.9 219.866,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1280.4 238.764,1280.4 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1089.65 238.764,1089.65 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,898.907 238.764,898.907 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,708.159 238.764,708.159 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,517.411 238.764,517.411 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,326.664 238.764,326.664 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,135.916 238.764,135.916 \"/>\n",
       "<path clip-path=\"url(#clip990)\" d=\"M126.205 1266.2 Q122.593 1266.2 120.765 1269.77 Q118.959 1273.31 118.959 1280.44 Q118.959 1287.54 120.765 1291.11 Q122.593 1294.65 126.205 1294.65 Q129.839 1294.65 131.644 1291.11 Q133.473 1287.54 133.473 1280.44 Q133.473 1273.31 131.644 1269.77 Q129.839 1266.2 126.205 1266.2 M126.205 1262.5 Q132.015 1262.5 135.07 1267.1 Q138.149 1271.69 138.149 1280.44 Q138.149 1289.16 135.07 1293.77 Q132.015 1298.35 126.205 1298.35 Q120.394 1298.35 117.316 1293.77 Q114.26 1289.16 114.26 1280.44 Q114.26 1271.69 117.316 1267.1 Q120.394 1262.5 126.205 1262.5 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M146.366 1291.8 L151.251 1291.8 L151.251 1297.68 L146.366 1297.68 L146.366 1291.8 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M174.283 1267.2 L162.477 1285.64 L174.283 1285.64 L174.283 1267.2 M173.056 1263.12 L178.936 1263.12 L178.936 1285.64 L183.866 1285.64 L183.866 1289.53 L178.936 1289.53 L178.936 1297.68 L174.283 1297.68 L174.283 1289.53 L158.681 1289.53 L158.681 1285.02 L173.056 1263.12 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M127.686 1075.45 Q124.075 1075.45 122.246 1079.02 Q120.441 1082.56 120.441 1089.69 Q120.441 1096.8 122.246 1100.36 Q124.075 1103.9 127.686 1103.9 Q131.32 1103.9 133.126 1100.36 Q134.954 1096.8 134.954 1089.69 Q134.954 1082.56 133.126 1079.02 Q131.32 1075.45 127.686 1075.45 M127.686 1071.75 Q133.496 1071.75 136.552 1076.36 Q139.63 1080.94 139.63 1089.69 Q139.63 1098.42 136.552 1103.02 Q133.496 1107.61 127.686 1107.61 Q121.876 1107.61 118.797 1103.02 Q115.742 1098.42 115.742 1089.69 Q115.742 1080.94 118.797 1076.36 Q121.876 1071.75 127.686 1071.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M147.848 1101.05 L152.732 1101.05 L152.732 1106.93 L147.848 1106.93 L147.848 1101.05 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M162.964 1072.37 L181.32 1072.37 L181.32 1076.31 L167.246 1076.31 L167.246 1084.78 Q168.264 1084.43 169.283 1084.27 Q170.302 1084.09 171.32 1084.09 Q177.107 1084.09 180.487 1087.26 Q183.866 1090.43 183.866 1095.85 Q183.866 1101.42 180.394 1104.53 Q176.922 1107.61 170.602 1107.61 Q168.427 1107.61 166.158 1107.23 Q163.913 1106.86 161.505 1106.12 L161.505 1101.42 Q163.589 1102.56 165.811 1103.11 Q168.033 1103.67 170.51 1103.67 Q174.514 1103.67 176.852 1101.56 Q179.19 1099.46 179.19 1095.85 Q179.19 1092.24 176.852 1090.13 Q174.514 1088.02 170.51 1088.02 Q168.635 1088.02 166.76 1088.44 Q164.908 1088.86 162.964 1089.74 L162.964 1072.37 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M126.529 884.705 Q122.918 884.705 121.089 888.27 Q119.283 891.812 119.283 898.941 Q119.283 906.048 121.089 909.612 Q122.918 913.154 126.529 913.154 Q130.163 913.154 131.968 909.612 Q133.797 906.048 133.797 898.941 Q133.797 891.812 131.968 888.27 Q130.163 884.705 126.529 884.705 M126.529 881.002 Q132.339 881.002 135.394 885.608 Q138.473 890.191 138.473 898.941 Q138.473 907.668 135.394 912.274 Q132.339 916.858 126.529 916.858 Q120.718 916.858 117.64 912.274 Q114.584 907.668 114.584 898.941 Q114.584 890.191 117.64 885.608 Q120.718 881.002 126.529 881.002 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M146.691 910.307 L151.575 910.307 L151.575 916.187 L146.691 916.187 L146.691 910.307 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M172.339 897.043 Q169.19 897.043 167.339 899.196 Q165.51 901.349 165.51 905.099 Q165.51 908.825 167.339 911.001 Q169.19 913.154 172.339 913.154 Q175.487 913.154 177.315 911.001 Q179.167 908.825 179.167 905.099 Q179.167 901.349 177.315 899.196 Q175.487 897.043 172.339 897.043 M181.621 882.39 L181.621 886.65 Q179.862 885.816 178.056 885.376 Q176.274 884.937 174.514 884.937 Q169.885 884.937 167.431 888.062 Q165.001 891.187 164.653 897.506 Q166.019 895.492 168.079 894.427 Q170.139 893.339 172.616 893.339 Q177.825 893.339 180.834 896.511 Q183.866 899.659 183.866 905.099 Q183.866 910.423 180.718 913.64 Q177.57 916.858 172.339 916.858 Q166.343 916.858 163.172 912.274 Q160.001 907.668 160.001 898.941 Q160.001 890.747 163.89 885.886 Q167.778 881.002 174.329 881.002 Q176.089 881.002 177.871 881.349 Q179.676 881.696 181.621 882.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M127.593 693.958 Q123.982 693.958 122.154 697.522 Q120.348 701.064 120.348 708.194 Q120.348 715.3 122.154 718.865 Q123.982 722.407 127.593 722.407 Q131.228 722.407 133.033 718.865 Q134.862 715.3 134.862 708.194 Q134.862 701.064 133.033 697.522 Q131.228 693.958 127.593 693.958 M127.593 690.254 Q133.404 690.254 136.459 694.86 Q139.538 699.444 139.538 708.194 Q139.538 716.921 136.459 721.527 Q133.404 726.11 127.593 726.11 Q121.783 726.11 118.705 721.527 Q115.649 716.921 115.649 708.194 Q115.649 699.444 118.705 694.86 Q121.783 690.254 127.593 690.254 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M147.755 719.559 L152.64 719.559 L152.64 725.439 L147.755 725.439 L147.755 719.559 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M161.644 690.879 L183.866 690.879 L183.866 692.87 L171.32 725.439 L166.436 725.439 L178.241 694.814 L161.644 694.814 L161.644 690.879 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M126.783 503.21 Q123.172 503.21 121.343 506.775 Q119.538 510.317 119.538 517.446 Q119.538 524.553 121.343 528.117 Q123.172 531.659 126.783 531.659 Q130.417 531.659 132.223 528.117 Q134.052 524.553 134.052 517.446 Q134.052 510.317 132.223 506.775 Q130.417 503.21 126.783 503.21 M126.783 499.506 Q132.593 499.506 135.649 504.113 Q138.728 508.696 138.728 517.446 Q138.728 526.173 135.649 530.779 Q132.593 535.363 126.783 535.363 Q120.973 535.363 117.894 530.779 Q114.839 526.173 114.839 517.446 Q114.839 508.696 117.894 504.113 Q120.973 499.506 126.783 499.506 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M146.945 528.812 L151.829 528.812 L151.829 534.691 L146.945 534.691 L146.945 528.812 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M172.014 518.28 Q168.681 518.28 166.76 520.062 Q164.862 521.844 164.862 524.969 Q164.862 528.094 166.76 529.877 Q168.681 531.659 172.014 531.659 Q175.348 531.659 177.269 529.877 Q179.19 528.071 179.19 524.969 Q179.19 521.844 177.269 520.062 Q175.371 518.28 172.014 518.28 M167.339 516.289 Q164.329 515.548 162.64 513.488 Q160.973 511.428 160.973 508.465 Q160.973 504.321 163.913 501.914 Q166.876 499.506 172.014 499.506 Q177.176 499.506 180.116 501.914 Q183.056 504.321 183.056 508.465 Q183.056 511.428 181.366 513.488 Q179.7 515.548 176.714 516.289 Q180.093 517.076 181.968 519.367 Q183.866 521.659 183.866 524.969 Q183.866 529.992 180.788 532.678 Q177.732 535.363 172.014 535.363 Q166.297 535.363 163.218 532.678 Q160.163 529.992 160.163 524.969 Q160.163 521.659 162.061 519.367 Q163.959 517.076 167.339 516.289 M165.626 508.905 Q165.626 511.59 167.292 513.094 Q168.982 514.599 172.014 514.599 Q175.024 514.599 176.714 513.094 Q178.426 511.59 178.426 508.905 Q178.426 506.219 176.714 504.715 Q175.024 503.21 172.014 503.21 Q168.982 503.21 167.292 504.715 Q165.626 506.219 165.626 508.905 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M126.876 312.463 Q123.265 312.463 121.436 316.027 Q119.63 319.569 119.63 326.699 Q119.63 333.805 121.436 337.37 Q123.265 340.912 126.876 340.912 Q130.51 340.912 132.316 337.37 Q134.144 333.805 134.144 326.699 Q134.144 319.569 132.316 316.027 Q130.51 312.463 126.876 312.463 M126.876 308.759 Q132.686 308.759 135.742 313.365 Q138.82 317.949 138.82 326.699 Q138.82 335.425 135.742 340.032 Q132.686 344.615 126.876 344.615 Q121.066 344.615 117.987 340.032 Q114.931 335.425 114.931 326.699 Q114.931 317.949 117.987 313.365 Q121.066 308.759 126.876 308.759 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M147.038 338.064 L151.922 338.064 L151.922 343.944 L147.038 343.944 L147.038 338.064 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M162.246 343.226 L162.246 338.967 Q164.005 339.8 165.811 340.24 Q167.616 340.68 169.352 340.68 Q173.982 340.68 176.413 337.578 Q178.866 334.453 179.214 328.111 Q177.871 330.101 175.811 331.166 Q173.751 332.231 171.251 332.231 Q166.065 332.231 163.033 329.106 Q160.024 325.958 160.024 320.518 Q160.024 315.194 163.172 311.977 Q166.32 308.759 171.552 308.759 Q177.547 308.759 180.695 313.365 Q183.866 317.949 183.866 326.699 Q183.866 334.87 179.977 339.754 Q176.112 344.615 169.561 344.615 Q167.802 344.615 165.996 344.268 Q164.19 343.921 162.246 343.226 M171.552 328.574 Q174.7 328.574 176.528 326.421 Q178.38 324.268 178.38 320.518 Q178.38 316.791 176.528 314.639 Q174.7 312.463 171.552 312.463 Q168.403 312.463 166.552 314.639 Q164.723 316.791 164.723 320.518 Q164.723 324.268 166.552 326.421 Q168.403 328.574 171.552 328.574 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M117.501 149.261 L125.14 149.261 L125.14 122.896 L116.83 124.562 L116.83 120.303 L125.093 118.636 L129.769 118.636 L129.769 149.261 L137.408 149.261 L137.408 153.196 L117.501 153.196 L117.501 149.261 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M146.853 147.317 L151.737 147.317 L151.737 153.196 L146.853 153.196 L146.853 147.317 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M171.922 121.715 Q168.311 121.715 166.482 125.28 Q164.677 128.822 164.677 135.951 Q164.677 143.058 166.482 146.622 Q168.311 150.164 171.922 150.164 Q175.556 150.164 177.362 146.622 Q179.19 143.058 179.19 135.951 Q179.19 128.822 177.362 125.28 Q175.556 121.715 171.922 121.715 M171.922 118.011 Q177.732 118.011 180.788 122.618 Q183.866 127.201 183.866 135.951 Q183.866 144.678 180.788 149.284 Q177.732 153.868 171.922 153.868 Q166.112 153.868 163.033 149.284 Q159.978 144.678 159.978 135.951 Q159.978 127.201 163.033 122.618 Q166.112 118.011 171.922 118.011 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M16.4842 1228.95 L16.4842 1198.91 L21.895 1198.91 L21.895 1222.52 L35.9632 1222.52 L35.9632 1199.89 L41.3741 1199.89 L41.3741 1222.52 L58.5933 1222.52 L58.5933 1198.33 L64.0042 1198.33 L64.0042 1228.95 L16.4842 1228.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M33.8307 1167.37 Q33.2578 1168.35 33.0032 1169.53 Q32.7167 1170.68 32.7167 1172.08 Q32.7167 1177.04 35.9632 1179.71 Q39.1779 1182.36 45.2253 1182.36 L64.0042 1182.36 L64.0042 1188.24 L28.3562 1188.24 L28.3562 1182.36 L33.8944 1182.36 Q30.6479 1180.51 29.0883 1177.55 Q27.4968 1174.59 27.4968 1170.36 Q27.4968 1169.75 27.5923 1169.02 Q27.656 1168.29 27.8151 1167.4 L33.8307 1167.37 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M33.8307 1141.71 Q33.2578 1142.7 33.0032 1143.88 Q32.7167 1145.02 32.7167 1146.42 Q32.7167 1151.39 35.9632 1154.06 Q39.1779 1156.7 45.2253 1156.7 L64.0042 1156.7 L64.0042 1162.59 L28.3562 1162.59 L28.3562 1156.7 L33.8944 1156.7 Q30.6479 1154.86 29.0883 1151.9 Q27.4968 1148.94 27.4968 1144.7 Q27.4968 1144.1 27.5923 1143.37 Q27.656 1142.63 27.8151 1141.74 L33.8307 1141.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M32.4621 1123.19 Q32.4621 1127.9 36.1542 1130.64 Q39.8145 1133.37 46.212 1133.37 Q52.6095 1133.37 56.3017 1130.67 Q59.9619 1127.93 59.9619 1123.19 Q59.9619 1118.51 56.2698 1115.77 Q52.5777 1113.03 46.212 1113.03 Q39.8781 1113.03 36.186 1115.77 Q32.4621 1118.51 32.4621 1123.19 M27.4968 1123.19 Q27.4968 1115.55 32.4621 1111.19 Q37.4273 1106.83 46.212 1106.83 Q54.9649 1106.83 59.9619 1111.19 Q64.9272 1115.55 64.9272 1123.19 Q64.9272 1130.86 59.9619 1135.22 Q54.9649 1139.55 46.212 1139.55 Q37.4273 1139.55 32.4621 1135.22 Q27.4968 1130.86 27.4968 1123.19 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M41.3104 1069.72 L58.7206 1069.72 L58.7206 1059.4 Q58.7206 1054.21 56.5881 1051.73 Q54.4238 1049.22 49.9996 1049.22 Q45.5436 1049.22 43.4429 1051.73 Q41.3104 1054.21 41.3104 1059.4 L41.3104 1069.72 M21.7677 1069.72 L36.0905 1069.72 L36.0905 1060.2 Q36.0905 1055.49 34.34 1053.2 Q32.5576 1050.87 28.9291 1050.87 Q25.3325 1050.87 23.5501 1053.2 Q21.7677 1055.49 21.7677 1060.2 L21.7677 1069.72 M16.4842 1076.14 L16.4842 1059.72 Q16.4842 1052.37 19.5397 1048.39 Q22.5952 1044.41 28.2289 1044.41 Q32.5894 1044.41 35.1675 1046.45 Q37.7456 1048.49 38.3822 1052.43 Q39.4007 1047.69 42.6472 1045.08 Q45.8619 1042.44 50.6998 1042.44 Q57.0655 1042.44 60.5348 1046.77 Q64.0042 1051.1 64.0042 1059.08 L64.0042 1076.14 L16.4842 1076.14 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M46.0847 1015.48 Q46.0847 1022.58 47.7079 1025.31 Q49.3312 1028.05 53.2461 1028.05 Q56.3653 1028.05 58.2114 1026.01 Q60.0256 1023.95 60.0256 1020.41 Q60.0256 1015.54 56.5881 1012.61 Q53.1188 1009.65 47.3897 1009.65 L46.0847 1009.65 L46.0847 1015.48 M43.6657 1003.8 L64.0042 1003.8 L64.0042 1009.65 L58.5933 1009.65 Q61.8398 1011.66 63.3994 1014.65 Q64.9272 1017.64 64.9272 1021.97 Q64.9272 1027.45 61.8716 1030.69 Q58.7843 1033.91 53.6281 1033.91 Q47.6125 1033.91 44.5569 1029.9 Q41.5014 1025.86 41.5014 1017.87 L41.5014 1009.65 L40.9285 1009.65 Q36.8862 1009.65 34.6901 1012.33 Q32.4621 1014.97 32.4621 1019.78 Q32.4621 1022.83 33.1941 1025.73 Q33.9262 1028.62 35.3903 1031.3 L29.9795 1031.3 Q28.7381 1028.08 28.1334 1025.06 Q27.4968 1022.04 27.4968 1019.17 Q27.4968 1011.44 31.5072 1007.62 Q35.5176 1003.8 43.6657 1003.8 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M29.4065 969.01 L34.9447 969.01 Q33.6716 971.492 33.035 974.166 Q32.3984 976.84 32.3984 979.704 Q32.3984 984.065 33.7352 986.261 Q35.072 988.425 37.7456 988.425 Q39.7826 988.425 40.9603 986.866 Q42.1061 985.306 43.1565 980.595 L43.6021 978.59 Q44.9389 972.352 47.3897 969.742 Q49.8086 967.1 54.1691 967.1 Q59.1344 967.1 62.0308 971.047 Q64.9272 974.962 64.9272 981.837 Q64.9272 984.701 64.3543 987.82 Q63.8132 990.908 62.6992 994.345 L56.6518 994.345 Q58.3387 991.099 59.198 987.948 Q60.0256 984.797 60.0256 981.709 Q60.0256 977.572 58.6251 975.344 Q57.1929 973.116 54.6147 973.116 Q52.2276 973.116 50.9545 974.739 Q49.6813 976.33 48.5037 981.773 L48.0262 983.81 Q46.8804 989.253 44.5251 991.672 Q42.138 994.091 38.0002 994.091 Q32.9713 994.091 30.2341 990.526 Q27.4968 986.961 27.4968 980.404 Q27.4968 977.158 27.9743 974.293 Q28.4517 971.429 29.4065 969.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M44.7161 927.283 L47.5806 927.283 L47.5806 954.209 Q53.6281 953.828 56.8109 950.581 Q59.9619 947.303 59.9619 941.478 Q59.9619 938.104 59.1344 934.953 Q58.3069 931.77 56.6518 928.651 L62.1899 928.651 Q63.5267 931.802 64.227 935.112 Q64.9272 938.423 64.9272 941.828 Q64.9272 950.358 59.9619 955.355 Q54.9967 960.321 46.5303 960.321 Q37.7774 960.321 32.6531 955.61 Q27.4968 950.868 27.4968 942.847 Q27.4968 935.653 32.1438 931.484 Q36.7589 927.283 44.7161 927.283 M42.9973 933.139 Q38.1912 933.203 35.3266 935.844 Q32.4621 938.454 32.4621 942.783 Q32.4621 947.685 35.2312 950.645 Q38.0002 953.573 43.0292 954.019 L42.9973 933.139 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M46.0847 901.47 Q46.0847 908.567 47.7079 911.305 Q49.3312 914.042 53.2461 914.042 Q56.3653 914.042 58.2114 912.005 Q60.0256 909.936 60.0256 906.403 Q60.0256 901.533 56.5881 898.605 Q53.1188 895.645 47.3897 895.645 L46.0847 895.645 L46.0847 901.47 M43.6657 889.789 L64.0042 889.789 L64.0042 895.645 L58.5933 895.645 Q61.8398 897.65 63.3994 900.642 Q64.9272 903.634 64.9272 907.963 Q64.9272 913.437 61.8716 916.684 Q58.7843 919.898 53.6281 919.898 Q47.6125 919.898 44.5569 915.888 Q41.5014 911.846 41.5014 903.857 L41.5014 895.645 L40.9285 895.645 Q36.8862 895.645 34.6901 898.319 Q32.4621 900.96 32.4621 905.766 Q32.4621 908.822 33.1941 911.718 Q33.9262 914.615 35.3903 917.288 L29.9795 917.288 Q28.7381 914.074 28.1334 911.05 Q27.4968 908.026 27.4968 905.162 Q27.4968 897.427 31.5072 893.608 Q35.5176 889.789 43.6657 889.789 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M33.7671 854.268 L14.479 854.268 L14.479 848.411 L64.0042 848.411 L64.0042 854.268 L58.657 854.268 Q61.8398 856.114 63.3994 858.947 Q64.9272 861.748 64.9272 865.694 Q64.9272 872.156 59.771 876.23 Q54.6147 880.272 46.212 880.272 Q37.8093 880.272 32.6531 876.23 Q27.4968 872.156 27.4968 865.694 Q27.4968 861.748 29.0564 858.947 Q30.5842 856.114 33.7671 854.268 M46.212 874.224 Q52.6732 874.224 56.3653 871.583 Q60.0256 868.909 60.0256 864.262 Q60.0256 859.615 56.3653 856.942 Q52.6732 854.268 46.212 854.268 Q39.7508 854.268 36.0905 856.942 Q32.3984 859.615 32.3984 864.262 Q32.3984 868.909 36.0905 871.583 Q39.7508 874.224 46.212 874.224 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M32.4621 822.535 Q32.4621 827.245 36.1542 829.983 Q39.8145 832.72 46.212 832.72 Q52.6095 832.72 56.3017 830.015 Q59.9619 827.277 59.9619 822.535 Q59.9619 817.856 56.2698 815.119 Q52.5777 812.382 46.212 812.382 Q39.8781 812.382 36.186 815.119 Q32.4621 817.856 32.4621 822.535 M27.4968 822.535 Q27.4968 814.896 32.4621 810.535 Q37.4273 806.175 46.212 806.175 Q54.9649 806.175 59.9619 810.535 Q64.9272 814.896 64.9272 822.535 Q64.9272 830.206 59.9619 834.566 Q54.9649 838.895 46.212 838.895 Q37.4273 838.895 32.4621 834.566 Q27.4968 830.206 27.4968 822.535 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M42.4881 746.115 L64.0042 746.115 L64.0042 751.971 L42.679 751.971 Q37.6183 751.971 35.1038 753.944 Q32.5894 755.918 32.5894 759.864 Q32.5894 764.607 35.6131 767.344 Q38.6368 770.081 43.8567 770.081 L64.0042 770.081 L64.0042 775.97 L28.3562 775.97 L28.3562 770.081 L33.8944 770.081 Q30.6797 767.981 29.0883 765.148 Q27.4968 762.283 27.4968 758.56 Q27.4968 752.417 31.3163 749.266 Q35.1038 746.115 42.4881 746.115 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M46.0847 718.233 Q46.0847 725.331 47.7079 728.068 Q49.3312 730.805 53.2461 730.805 Q56.3653 730.805 58.2114 728.768 Q60.0256 726.699 60.0256 723.166 Q60.0256 718.296 56.5881 715.368 Q53.1188 712.408 47.3897 712.408 L46.0847 712.408 L46.0847 718.233 M43.6657 706.552 L64.0042 706.552 L64.0042 712.408 L58.5933 712.408 Q61.8398 714.413 63.3994 717.405 Q64.9272 720.397 64.9272 724.726 Q64.9272 730.2 61.8716 733.447 Q58.7843 736.661 53.6281 736.661 Q47.6125 736.661 44.5569 732.651 Q41.5014 728.609 41.5014 720.62 L41.5014 712.408 L40.9285 712.408 Q36.8862 712.408 34.6901 715.082 Q32.4621 717.723 32.4621 722.53 Q32.4621 725.585 33.1941 728.482 Q33.9262 731.378 35.3903 734.052 L29.9795 734.052 Q28.7381 730.837 28.1334 727.813 Q27.4968 724.789 27.4968 721.925 Q27.4968 714.191 31.5072 710.371 Q35.5176 706.552 43.6657 706.552 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M20.1444 637.929 L26.9239 637.929 Q23.9002 641.176 22.4043 644.868 Q20.9083 648.528 20.9083 652.666 Q20.9083 660.814 25.9054 665.143 Q30.8707 669.471 40.2919 669.471 Q49.6813 669.471 54.6784 665.143 Q59.6436 660.814 59.6436 652.666 Q59.6436 648.528 58.1477 644.868 Q56.6518 641.176 53.6281 637.929 L60.3439 637.929 Q62.6355 641.303 63.7814 645.091 Q64.9272 648.847 64.9272 653.048 Q64.9272 663.838 58.3387 670.044 Q51.7183 676.251 40.2919 676.251 Q28.8336 676.251 22.2451 670.044 Q15.6248 663.838 15.6248 653.048 Q15.6248 648.783 16.7706 645.027 Q17.8846 641.24 20.1444 637.929 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M33.8307 607.597 Q33.2578 608.583 33.0032 609.761 Q32.7167 610.907 32.7167 612.307 Q32.7167 617.273 35.9632 619.946 Q39.1779 622.588 45.2253 622.588 L64.0042 622.588 L64.0042 628.476 L28.3562 628.476 L28.3562 622.588 L33.8944 622.588 Q30.6479 620.742 29.0883 617.782 Q27.4968 614.822 27.4968 610.589 Q27.4968 609.984 27.5923 609.252 Q27.656 608.52 27.8151 607.629 L33.8307 607.597 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M32.4621 589.073 Q32.4621 593.783 36.1542 596.52 Q39.8145 599.258 46.212 599.258 Q52.6095 599.258 56.3017 596.552 Q59.9619 593.815 59.9619 589.073 Q59.9619 584.394 56.2698 581.657 Q52.5777 578.919 46.212 578.919 Q39.8781 578.919 36.186 581.657 Q32.4621 584.394 32.4621 589.073 M27.4968 589.073 Q27.4968 581.434 32.4621 577.073 Q37.4273 572.713 46.212 572.713 Q54.9649 572.713 59.9619 577.073 Q64.9272 581.434 64.9272 589.073 Q64.9272 596.743 59.9619 601.104 Q54.9649 605.432 46.212 605.432 Q37.4273 605.432 32.4621 601.104 Q27.4968 596.743 27.4968 589.073 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M29.4065 540.279 L34.9447 540.279 Q33.6716 542.762 33.035 545.436 Q32.3984 548.109 32.3984 550.974 Q32.3984 555.334 33.7352 557.53 Q35.072 559.695 37.7456 559.695 Q39.7826 559.695 40.9603 558.135 Q42.1061 556.576 43.1565 551.865 L43.6021 549.86 Q44.9389 543.621 47.3897 541.011 Q49.8086 538.37 54.1691 538.37 Q59.1344 538.37 62.0308 542.316 Q64.9272 546.231 64.9272 553.106 Q64.9272 555.971 64.3543 559.09 Q63.8132 562.177 62.6992 565.615 L56.6518 565.615 Q58.3387 562.368 59.198 559.217 Q60.0256 556.066 60.0256 552.979 Q60.0256 548.841 58.6251 546.613 Q57.1929 544.385 54.6147 544.385 Q52.2276 544.385 50.9545 546.009 Q49.6813 547.6 48.5037 553.043 L48.0262 555.08 Q46.8804 560.522 44.5251 562.941 Q42.138 565.36 38.0002 565.36 Q32.9713 565.36 30.2341 561.796 Q27.4968 558.231 27.4968 551.674 Q27.4968 548.428 27.9743 545.563 Q28.4517 542.698 29.4065 540.279 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M29.4065 506.318 L34.9447 506.318 Q33.6716 508.801 33.035 511.475 Q32.3984 514.148 32.3984 517.013 Q32.3984 521.373 33.7352 523.569 Q35.072 525.734 37.7456 525.734 Q39.7826 525.734 40.9603 524.174 Q42.1061 522.615 43.1565 517.904 L43.6021 515.899 Q44.9389 509.66 47.3897 507.05 Q49.8086 504.409 54.1691 504.409 Q59.1344 504.409 62.0308 508.355 Q64.9272 512.27 64.9272 519.145 Q64.9272 522.01 64.3543 525.129 Q63.8132 528.216 62.6992 531.654 L56.6518 531.654 Q58.3387 528.407 59.198 525.256 Q60.0256 522.105 60.0256 519.018 Q60.0256 514.88 58.6251 512.652 Q57.1929 510.424 54.6147 510.424 Q52.2276 510.424 50.9545 512.048 Q49.6813 513.639 48.5037 519.082 L48.0262 521.119 Q46.8804 526.561 44.5251 528.98 Q42.138 531.399 38.0002 531.399 Q32.9713 531.399 30.2341 527.834 Q27.4968 524.27 27.4968 517.713 Q27.4968 514.466 27.9743 511.602 Q28.4517 508.737 29.4065 506.318 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M16.4842 474.108 L16.4842 444.062 L21.895 444.062 L21.895 467.679 L35.9632 467.679 L35.9632 445.048 L41.3741 445.048 L41.3741 467.679 L58.5933 467.679 L58.5933 443.489 L64.0042 443.489 L64.0042 474.108 L16.4842 474.108 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M42.4881 403.544 L64.0042 403.544 L64.0042 409.4 L42.679 409.4 Q37.6183 409.4 35.1038 411.374 Q32.5894 413.347 32.5894 417.294 Q32.5894 422.036 35.6131 424.774 Q38.6368 427.511 43.8567 427.511 L64.0042 427.511 L64.0042 433.399 L28.3562 433.399 L28.3562 427.511 L33.8944 427.511 Q30.6797 425.41 29.0883 422.578 Q27.4968 419.713 27.4968 415.989 Q27.4968 409.846 31.3163 406.695 Q35.1038 403.544 42.4881 403.544 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M18.2347 386.07 L28.3562 386.07 L28.3562 374.007 L32.9077 374.007 L32.9077 386.07 L52.2594 386.07 Q56.6199 386.07 57.8613 384.893 Q59.1026 383.683 59.1026 380.023 L59.1026 374.007 L64.0042 374.007 L64.0042 380.023 Q64.0042 386.802 61.4897 389.38 Q58.9434 391.958 52.2594 391.958 L32.9077 391.958 L32.9077 396.255 L28.3562 396.255 L28.3562 391.958 L18.2347 391.958 L18.2347 386.07 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M33.8307 345.648 Q33.2578 346.635 33.0032 347.812 Q32.7167 348.958 32.7167 350.359 Q32.7167 355.324 35.9632 357.997 Q39.1779 360.639 45.2253 360.639 L64.0042 360.639 L64.0042 366.527 L28.3562 366.527 L28.3562 360.639 L33.8944 360.639 Q30.6479 358.793 29.0883 355.833 Q27.4968 352.873 27.4968 348.64 Q27.4968 348.035 27.5923 347.303 Q27.656 346.571 27.8151 345.68 L33.8307 345.648 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M32.4621 327.124 Q32.4621 331.834 36.1542 334.572 Q39.8145 337.309 46.212 337.309 Q52.6095 337.309 56.3017 334.603 Q59.9619 331.866 59.9619 327.124 Q59.9619 322.445 56.2698 319.708 Q52.5777 316.97 46.212 316.97 Q39.8781 316.97 36.186 319.708 Q32.4621 322.445 32.4621 327.124 M27.4968 327.124 Q27.4968 319.485 32.4621 315.124 Q37.4273 310.764 46.212 310.764 Q54.9649 310.764 59.9619 315.124 Q64.9272 319.485 64.9272 327.124 Q64.9272 334.794 59.9619 339.155 Q54.9649 343.484 46.212 343.484 Q37.4273 343.484 32.4621 339.155 Q27.4968 334.794 27.4968 327.124 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M58.657 295.391 L77.5631 295.391 L77.5631 301.279 L28.3562 301.279 L28.3562 295.391 L33.7671 295.391 Q30.5842 293.545 29.0564 290.744 Q27.4968 287.911 27.4968 283.996 Q27.4968 277.503 32.6531 273.461 Q37.8093 269.387 46.212 269.387 Q54.6147 269.387 59.771 273.461 Q64.9272 277.503 64.9272 283.996 Q64.9272 287.911 63.3994 290.744 Q61.8398 293.545 58.657 295.391 M46.212 275.466 Q39.7508 275.466 36.0905 278.14 Q32.3984 280.781 32.3984 285.428 Q32.3984 290.075 36.0905 292.749 Q39.7508 295.391 46.212 295.391 Q52.6732 295.391 56.3653 292.749 Q60.0256 290.075 60.0256 285.428 Q60.0256 280.781 56.3653 278.14 Q52.6732 275.466 46.212 275.466 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M67.3143 244.847 Q73.68 247.33 75.6216 249.685 Q77.5631 252.04 77.5631 255.987 L77.5631 260.666 L72.6615 260.666 L72.6615 257.228 Q72.6615 254.809 71.5157 253.473 Q70.3699 252.136 66.1048 250.512 L63.4312 249.462 L28.3562 263.88 L28.3562 257.674 L56.238 246.534 L28.3562 235.394 L28.3562 229.187 L67.3143 244.847 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip992)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"280.231,85.838 543.47,118.999 697.455,147.61 806.709,182.264 891.453,204.903 960.694,221.323 1019.24,246.153 1114.68,309.136 1190.89,366.391 1254.33,402.974 1308.68,454.576 1398.45,526.719 1454.13,581.085 1531.9,649.462 1596.43,725.293 1671.55,797.597 1734.25,855.265 1808.95,909.575 1877.06,1001.34 1944.41,1069.58 2013.65,1150.6 2085.52,1223.76 2154.21,1265.17 2223.91,1330.51 2292.39,1372.3 \"/>\n",
       "<path clip-path=\"url(#clip990)\" d=\"M2015.94 196.379 L2281.66 196.379 L2281.66 92.6992 L2015.94 92.6992  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2015.94,196.379 2281.66,196.379 2281.66,92.6992 2015.94,92.6992 2015.94,196.379 \"/>\n",
       "<polyline clip-path=\"url(#clip990)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2039.64,144.539 2181.83,144.539 \"/>\n",
       "<path clip-path=\"url(#clip990)\" d=\"M2219.37 164.227 Q2217.57 168.856 2215.85 170.268 Q2214.14 171.68 2211.27 171.68 L2207.87 171.68 L2207.87 168.115 L2210.37 168.115 Q2212.13 168.115 2213.1 167.282 Q2214.07 166.449 2215.25 163.347 L2216.02 161.403 L2205.53 135.893 L2210.04 135.893 L2218.15 156.171 L2226.25 135.893 L2230.76 135.893 L2219.37 164.227 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip990)\" d=\"M2238.05 157.884 L2245.69 157.884 L2245.69 131.518 L2237.38 133.185 L2237.38 128.926 L2245.65 127.259 L2250.32 127.259 L2250.32 157.884 L2257.96 157.884 L2257.96 161.819 L2238.05 161.819 L2238.05 157.884 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "plot(curva_aprendizado.parameter_values,\n",
    "     curva_aprendizado.measurements,\n",
    "     xlab = curva_aprendizado.parameter_name,\n",
    "     xscale = curva_aprendizado.parameter_scale,\n",
    "     ylab = \"Erro Baseado na Cross Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo em disco\n",
    "MLJ.save(\"modelo/modelo_rede_neural.jlso\", mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega o modelo do disco\n",
    "mach2 = machine(\"modelo/modelo_rede_neural.jlso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float32}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.119, Iris-versicolor=>0.518, Iris-virginica=>0.363)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0487, Iris-versicolor=>0.468, Iris-virginica=>0.483)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.889, Iris-versicolor=>0.108, Iris-virginica=>0.00284)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0115, Iris-versicolor=>0.402, Iris-virginica=>0.586)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.00762, Iris-versicolor=>0.375, Iris-virginica=>0.617)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.866, Iris-versicolor=>0.129, Iris-virginica=>0.00465)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0435, Iris-versicolor=>0.485, Iris-virginica=>0.471)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0638, Iris-versicolor=>0.515, Iris-virginica=>0.422)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0141, Iris-versicolor=>0.43, Iris-virginica=>0.556)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.151, Iris-versicolor=>0.524, Iris-virginica=>0.325)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0259, Iris-versicolor=>0.442, Iris-virginica=>0.532)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0846, Iris-versicolor=>0.503, Iris-virginica=>0.412)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0325, Iris-versicolor=>0.456, Iris-virginica=>0.512)\n",
       " ⋮\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0299, Iris-versicolor=>0.451, Iris-virginica=>0.519)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.897, Iris-versicolor=>0.101, Iris-virginica=>0.00245)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0148, Iris-versicolor=>0.414, Iris-virginica=>0.571)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.01, Iris-versicolor=>0.389, Iris-virginica=>0.601)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.02, Iris-versicolor=>0.422, Iris-virginica=>0.558)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.192, Iris-versicolor=>0.504, Iris-virginica=>0.304)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0108, Iris-versicolor=>0.4, Iris-virginica=>0.589)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.901, Iris-versicolor=>0.0968, Iris-virginica=>0.00225)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.00906, Iris-versicolor=>0.385, Iris-virginica=>0.606)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0172, Iris-versicolor=>0.421, Iris-virginica=>0.562)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.904, Iris-versicolor=>0.0937, Iris-virginica=>0.00204)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.905, Iris-versicolor=>0.0928, Iris-virginica=>0.002)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faz previsões com dados de teste\n",
    "yhat = predict(mach2, X[test,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float32}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.119, Iris-versicolor=>0.518, Iris-virginica=>0.363)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0487, Iris-versicolor=>0.468, Iris-virginica=>0.483)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.889, Iris-versicolor=>0.108, Iris-virginica=>0.00284)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0115, Iris-versicolor=>0.402, Iris-virginica=>0.586)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.00762, Iris-versicolor=>0.375, Iris-virginica=>0.617)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza algumas previsões\n",
    "yhat[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36286193f0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para obter a probabilidade de \"Iris-virginica\" na primeira previsão\n",
    "pdf(yhat[1], \"Iris-virginica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45×3 Matrix{Float32}:\n",
       " 0.119215    0.517923   0.362862\n",
       " 0.0487063   0.467895   0.483399\n",
       " 0.889354    0.107802   0.002844\n",
       " 0.0115457   0.402469   0.585985\n",
       " 0.00762037  0.375344   0.617036\n",
       " 0.86612     0.129229   0.00465095\n",
       " 0.0435229   0.485086   0.471392\n",
       " 0.0638348   0.51464    0.421525\n",
       " 0.0141317   0.429722   0.556146\n",
       " 0.151253    0.523735   0.325012\n",
       " 0.0259328   0.442118   0.531949\n",
       " 0.084559    0.503251   0.41219\n",
       " 0.0325244   0.45567    0.511806\n",
       " ⋮                      \n",
       " 0.0298583   0.450793   0.519349\n",
       " 0.896589    0.100959   0.0024525\n",
       " 0.0148119   0.414226   0.570962\n",
       " 0.0100021   0.388967   0.60103\n",
       " 0.0200257   0.421652   0.558322\n",
       " 0.191731    0.504032   0.304237\n",
       " 0.0108088   0.400388   0.588803\n",
       " 0.900947    0.0968066  0.00224659\n",
       " 0.00906411  0.385432   0.605504\n",
       " 0.0171618   0.42077    0.562068\n",
       " 0.904281    0.0936823  0.0020363\n",
       " 0.905216    0.0927886  0.00199537"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtém as previsões de cada linha com as probabilidades para cada uma das 3 classes\n",
    "L = levels(y)\n",
    "pdf(yhat, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.6",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
